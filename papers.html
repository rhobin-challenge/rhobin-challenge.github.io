<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>3rd RHOBIN Workshop@CVPR25</title>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <link href="data/default.css" rel="stylesheet" type="text/css" />
    <link rel="icon" type="image/png" href="./data/rhobin-logo.png" />
    <meta property='og:title' content='RHOBIN Workshop: Reconstruction of Human-Object Interactions @ CVPR24'/>
    <meta property='og:url' content='https://rhobin-challenge.github.io/' />
    <meta property="og:type" content="website" />
    
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script>
        function moreOrLess(tableData) {
            obj = tableData.find("span");
            button = tableData.find("button");
            console.log(obj.hasClass("hidden"))
            if (obj.hasClass("hidden")) {
                obj.slideDown("fast");
                obj.removeClass("hidden");
                button.html("Hide Abstract");
            } else {
                obj.slideUp("fast");
                obj.addClass("hidden");
                button.html("Show Abstract");
            }
        }
    </script>
  </head>

  <body>
    <div id="header">
        <div id="logo">
          <h1>
              <center>
                  <span style="font-size:50%;color:#777;font-weight:normal">The third Workshop on</span><br>
                  Reconstruction of Human-Object Interactions (RHOBIN)
              </center>
          </h1><br>
          <div class="my-image-box-header">
              <center><img src="./data/rhobin-logo-no-bg.png" alt="rhobin-logo" width="128" height="168"></center>
          </div>
          <h2>
            <center>
                <span style="font-size:92%;color:#777;font-weight:normal">June 12, 08<sup>50</sup> - 12<sup>30</sup> @ Room 212, 
                    <a href="https://cvpr2025.thecvf.com/" target="_blank">CVPR 2025</a>, Nashville TN, USA &amp; <a href="https://cvpr.thecvf.com/virtual/2025/workshop/32335" target="_blank" style="text-decoration: underline;">online</a></span><br>
                <span style="font-size:130%;color:#777;font-weight:bold">Music City Center</span>
            </center>
          </h2><br>
      </div>
  
    <div id="menu">
    <center>
      <ul>
        <li class="first"><a href="./index.html" accesskey="1">Home</a></li>
      </ul>
    </center>
</div>

<button onclick="topFunction()" id="myBtn" title="Go to top">Top</button>

<script>
//Get the button
var mybutton = document.getElementById("myBtn");

// When the user scrolls down 20px from the top of the document, show the button
window.onscroll = function() {scrollFunction()};

function scrollFunction() {
      if (document.body.scrollTop > 20 || document.documentElement.scrollTop > 20) {
              mybutton.style.display = "block";
                } else {
                        mybutton.style.display = "none";
                          }
}

// When the user clicks on the button, scroll to the top of the document
function topFunction() {
      document.body.scrollTop = 0;
        document.documentElement.scrollTop = 0;
}
</script>
</div>


        <div id="content">
<!-- <p>
<ul>
<li>
    We are hosting <a href="https://app.gather.town/app/zDHzSsCHaRrEvEq4/ECCV%20VOLI%20Workshop" target="_blank">the poster session</a> on GatherTown (check out <a href="./gathertown_instruction.pdf" target="_blank">the GatherTown instructions</a> on how to join).
</li>
</ul>
</p> -->
<!-- <div style="margin-top: 100px;">
<br>
    <strong>
    <p class="poster_title">
    We are happy to announce the winners of the RHOBIN Challenge!
    Thanks to the generous sponsorhsip of <a href="https://www.adobe.com"></a><span style="font-size: 24px; color: #ff0000">Adobe
         </span></a>
    each winning team will receive a gift of 500 USD &#127881;!
</strong>
</div> -->

<div style="margin-bottom: 0px;">
<center>
    <h2 style="margin-bottom: 15px">Paper Presentations</h2>
    <table>
        <tr class="row_type_A">
            <td width="20px" class="paper_id">1</td>
            <td width="200px">
                <div class="author-container">
                    <span class="author">
                        Romain Brégier<br>
                        Fabien Baradel<br>
                        Thomas Lucas<br>
                        Salma Galaaoui<br>
                        Matthieu Armando<br>
                        Philippe Weinzaepfel<br>
                        Grégory Rogez
                    </span>
                </div>
                <!-- <div style="text-align: center;">
                    <a href="" target="_blank">[PDF]</a>
                    <a href="https://youtu.be/" target="_blank">[Video]</a>
                </div> -->
            </td>


            <td id="paper1">
                <p class="poster_title">
                   CondiMen: Conditional Multi-Person Mesh Recovery
                </p> 
                <span class="hidden">
                    <p><strong>Abstract:</strong>
                        Multi-person human mesh recovery (HMR) consists in detecting all individuals in a given input image, and predicting the body shape, pose, and 3D location for each detected person. The dominant approaches to this task rely on neural networks trained to output a single prediction for each detected individual. In contrast, we propose CondiMen, a method that outputs a joint parametric distribution over likely poses, body shapes, intrinsics and distances to the camera, using a Bayesian network. This approach offers several advantages. First, a probability distribution can handle some inherent ambiguities of this task – such as the uncertainty between a person’s size and their distance to the camera, or more generally the loss of information that occurs when projecting 3D data onto a 2D image. Second, the output distribution can be combined with additional information to produce better predictions, by using e.g. known camera or body shape parameters, or by exploiting multi-view observations. Third, one can efficiently extract the most likely predictions from this output distribution, making the proposed approach suitable for real-time applications. Empirically we find that our model i) achieves performance on par with or better than the state-of-the-art, ii) captures uncertainties and correlations inherent in pose estimation and iii) can exploit additional information at test time, such as multi-view consistency or body shape priors. CondiMen spices up the modeling of ambiguity, using just the right in- gredients on hand.
                    </p>
                </span>
                <div style="text-align: center;">
                    <button onclick="moreOrLess($('#paper1'))">Show Abstract</button>
                </div>
            </td>
        </tr>

        <tr class="row_type_B">
                <td width="20px" class="paper_id">2</td>
                <td width="200px">
                    <div class="author-container">
                        <span class="author">
                            Ayce Idil Aytekin<br>
                            Chuqiao Li<br>
                            Diogo Luvizon<br>
                            Rishabh Dabral<br>
                            Martin R. Oswald<br>
                            Marc Habermann<br>
                            Christian Theobalt 
                        </span>
                    </div>
                    <!-- <div style="text-align: center;">
                        <a href="" target="_blank">[PDF]</a>
                        <a href="https://youtu.be/" target="_blank">[Video]</a>
                    </div> -->
                </td>
                <td id="paper2">
                    <p class="poster_title">
                    Physics-based Human Pose Estimation from a Single Moving RGB Camera
                    </p>
                    <span class="hidden">
                        <p><strong>Abstract:</strong>
                            Most monocular and physics-based human pose tracking methods, while achieving state-of-the-art results, suffer from artifacts when the scene does not have a strictly flat ground plane or when the camera is moving. Moreover, these methods are often evaluated on in-the-wild real world videos without ground-truth data or on synthetic datasets, which fail to model the real world light transport, camera motion, and pose-induced appearance and geometry changes. To tackle these two problems, we introduce MoviCam, the first non-synthetic dataset containing ground-truth camera trajectories of a dynamically moving monocular RGB camera, scene geometry, and 3D human motion with human-scene contact labels. Additionally, we propose PhysDynPose, a physics-based method that incorporates scene geometry and physical constraints for more accurate human motion tracking in case of camera motion and non-flat scenes. More precisely, we use a state-of-the-art kinematics estimator to obtain the human pose and a robust SLAM method to capture the dynamic camera trajectory, enabling the recovery of the human pose in the world frame. We then refine the kinematic pose estimate using our scene-aware physics optimizer. From our new benchmark, we found that even state-of-the-art methods struggle with this inherently challenging setting, i.e. a moving camera and non-planar environments, while our method robustly estimates both human and camera poses in world coordinates.
                        </p>
                    </span>
                    <div style="text-align: center;">
                        <button onclick="moreOrLess($('#paper2'))">Show Abstract</button>
                    </div>
                </td>
        </tr>

        <tr class="row_type_A">
            <td width="20px" , class="paper_id">3</td>
            <td width="200px">
                <div class="author-container">
                    <span class="author">
                        Xiyuan Kang<br>
                        Yi Yuan<br>
                        Xu Dong<br>
                        Muhammad Awais<br>
                        Lilian Tang<br>
                        Josef Kittler<br>
                        Zhenhua Feng
                    </span>
                </div>
                <!-- <div style="text-align: center;">
                    <a href="" target="_blank">[PDF]</a>
                    <a href="https://youtu.be/" target="_blank">[Video]</a>
                </div> -->
            </td>
            <td id="paper3">
                <p class="poster_title">
                    Short-term 3D Human Mesh Recovery with Virtual Markers Disentanglement
                </p>
                <span class="hidden">
                    <p><strong>Abstract:</strong>
                        Human mesh recovery is a fundamental and challenging task in computer vision. Existing image-based methods suffer from depth ambiguity due to the absence of explicit 3D contextual information. Conversely, video-based methods leverage multi-view input and temporal consistency to improve stability but struggle with capturing fine-grained spatial details and have high computational costs. To effectively combine the spatial precision of image-based techniques with the temporal robustness of video-based approaches, we propose a temporal Transformer framework augmented with the state-of-the-art image-based reconstruction model, Virtual Markers. Specifically, we introduce a novel disentanglement module designed to explicitly separate Virtual Markers into distinct pose and shape representations. Leveraging short-term temporal context, the proposed module enhances the consistency of body shape and pose coherence across frames, ensuring both spatial accuracy and computational efficiency. Experimental results demonstrate that the proposed method significantly enhances the performance and interpretability of virtual markers. Our model achieves state-of-the-art results on two widely used benchmarking datasets, outperforming previous image-based approaches across different evaluation metrics.
                    </p>
                </span>
                <div style="text-align: center;">
                    <button onclick="moreOrLess($('#paper3'))">Show Abstract</button>
                </div> 
            </td>
        </tr>
    
        <tr class="row_type_B">
                <td width="20px" class="paper_id">4</td>
                <td width="200px">
                    <div class="author-container">
                        <span class="author">
                            Sonain Jamil
                        </span>
                    </div>
                    <!-- <div style="text-align: center;">
                        <a href="" target="_blank">[PDF]</a>
                        <a href="https://youtu.be/" target="_blank">[Video]</a>
                    </div> -->
                </td>
                <td id="paper2">
                    <p class="poster_title">
                    PoseSynViT: Lightweight and Scalable Vision Transformers for Human Pose Estimation
                    </p>
                    <span class="hidden">
                        <p><strong>Abstract:</strong>
                            Vision transformers (ViTs) have consistently delivered outstanding results in visual recognition tasks without needing specialized domain knowledge. Nevertheless, their application in human pose estimation (HPE) tasks remains underexplored. This paper introduces PoseSynViT, a new lightweight ViT model that surpasses ViTPose in several areas, including simplicity of model architecture, scalability, training versatility, and ease of knowledge transfer. Our model uses ViTs as backbones to extract features for HPE and integrates a lightweight decoder. It scales efficiently from 10M to 1B parameters, taking advantage of the inherent scalability and high parallelism of transformers, setting a new benchmark between throughput and performance. PoseSynViT is highly adaptable, supporting various attention mechanisms, input resolutions, and training approaches, and is capable of handling multiple HPE tasks. Additionally, we demonstrate that knowledge from larger models can be seamlessly transferred to smaller ones through a straightforward knowledge token. Experimental results on the MS COCO benchmark show that PoseSynViT outperforms current methods, with our largest model setting a new state-of-the-art performance of 84.3 AP on the MS COCO test dataset.
                        </p>
                    </span>
                    <div style="text-align: center;">
                        <button onclick="moreOrLess($('#paper2'))">Show Abstract</button>
                    </div>
                </td>
        </tr>
    </table>
</center>
</div>

<!-- <center>
    <h2>Abstract Presentations</h2>
    <table>

        <tr class="row_type_B">

            <td width="200px">
                <div style="text-align: center;">
                    <strong><span class="author">
                        Aditya Prakash (University of Illinois Urbana-Champaign)*; Matthew Chang (UIUC); Matthew Jin (University of Illinois at Urbana-Champaign); Saurabh Gupta (UIUC)                         </div>
                <div style="text-align: center;">
                    <a href="" target="_blank">[PDF]</a>
                    <a href="https://youtu.be/" target="_blank">[Video]</a>
                </div>
            </td>
            <td id="abstract1">
                <p class="poster_title">
                    Learning Hand-Held Object Reconstruction from In-The-Wild Videos
                    <span class="hidden">
                    <p><strong>Abstract:</strong>
                        Prior works for reconstructing hand-held objects from a single image rely on direct 3D shape supervision which is challenging to gather in real world at scale. Consequently, these approaches do not generalize well when presented with novel objects in in-the-wild settings. While 3D supervision is a major bottleneck, there is an abundance of in-the-wild raw video data showing hand-object interactions. In this paper, we automatically extract 3D supervision (via multiview 2D supervision) from such raw video data (e.g. VISOR) to scale up the learning of models for hand-held object reconstruction. This requires tackling two key challenges: unknown camera pose and occlusion. For the former, we use hand pose (from existing techniques) as a proxy for object pose. For the latter, we learn data-driven 3D shape priors using synthetic objects from the ObMan dataset. We use these indirect 3D cues to train occupancy networks that predict the 3D shape of objects from a single RGB image. Our experiments on the MOW dataset show the effectiveness of these supervisory signals in predicting coherent 3D shapes and generalizing better to in-the-wild images compared to baselines. Since our approach does not require 3D supervision, we can also scale it up by incorporating additional datasets.
                    </p>
                </span>
                <div style="text-align: center;">
                </div>
            </td>
        </tr>


        <tr class="row_type_B">
            <td width="200px">
                <div style="text-align: center;">
                    <strong><span class="author">
                        Yi-Ling Qiao (University of Maryland, College Park)*; Shutong Zhang (University of Toronto); Guanglei Zhu (University of Toronto); Eric Heiden (NVIDIA); Dylan Turpin (University of Toronto); Ming C Lin (UMD-CP & UNC-CH ); Miles Macklin (NVIDIA); Animesh Garg (University of Toronto, Vector Institute, Nvidia)
                    </div>
                <div style="text-align: center;">
                    <a href="" target="_blank">[PDF]</a>
                    <a href="https://youtu.be/" target="_blank">[Video]</a>
                </div>
            </td>
            <td id="abstract2">
                <p class="poster_title">
                    HandyPriors: Physically Consistent Perception of Hand-Object Interactions with Differentiable Priors                         <span class="hidden">

                </span>
                <div style="text-align: center;">
                </div>
            </td>
        </tr>

    </table>
</center> -->

<center>
    <h2>Challenge Winners</h2><br>
    <table>
        <!--Human Reconstruction track-->
        <tr class="row_type_A">
            <td width=200px" class="paper_id" rowspan="2">Human Reconstruction track</td>
            <td width=50px"><div style="text-align: center;">1st place</div></td>
            <td width="600px">
                <div style="text-align: center;">
                    <!-- <strong><span class="author">
                        Fabien Baradel, Thomas Lucas, Matthieu Armando, Romain Brégier,<br>
                        Salma Galaaoui, Philippe Weinzaepfel, Gregory Rogez<br>
                        from NAVER LABS
                    </span></strong> -->
                </div>
            </td>
        </tr>
        <tr class="row_type_A">
            <td width=50px"><div style="text-align: center;">2nd place</div></td>
            <td width="600px">
                <div style="text-align: center;">
                    <!-- <strong><span class="author">
                        Yangxu Yan<sup>1</sup>, Anlong Ming<sup>1</sup>, Huadong Ma<sup>1</sup>, Zhaowen Lin<sup>1</sup>,<br>
                        Weihong Yao<sup>2</sup> from <br>
                        <sup>1</sup>Beijing University of Posts and Telecommunications and<br>
                        <sup>2</sup>Shanghai Vision Era Co. Ltd.
                    </span></strong> -->
                </div>
            </td>
        </tr>

        <!--Joint Human-Object Reconstruction track-->
        <tr class="row_type_B">
            <td width="200px" class="paper_id" rowspan="2">Joint Human-Object Reconstruction track</td>
            <td width=50px"><div style="text-align: center;">1st place</div></td>
            <td width="600px">
                <div style="text-align: center;">
                    <!-- <strong><span class="author">
                        Yuyang Jing<sup>1</sup>, Anlong Ming<sup>1</sup>, Yongchang Zhang<sup>1</sup>, Xiaohai Yu<sup>1</sup>,<br>
                        Huadong Ma<sup>1</sup>, Weihong Yao<sup>2</sup> from <br>
                        <sup>1</sup>Beijing University of Posts and Telecommunications and<br>
                        <sup>2</sup>Shanghai Vision Era Co. Ltd.
                    </span></strong> -->
                </div>
            </td>
        </tr>
        <tr class="row_type_B">
            <td width=50px"><div style="text-align: center;">2nd place</div></td>
            <td width="600px">
                <div style="text-align: center;">
                    <!-- <strong><span class="author">
                        Feng Xue<sup>1</sup>, Ke Han<sup>1</sup>, Tianrui Ye<sup>2</sup><br>
                        from <sup>1</sup>University of Trento and<br>
                        <sup>2</sup>University of North Carolina at Chapel Hill
                    </span></strong> -->
                </div>
            </td>
        </tr>

        <!--3D Contact Estimation track-->
        <tr class="row_type_A">
            <td width="200px" class="paper_id" rowspan="3">3D Contact Estimation track</td>
            <td width=50px"><div style="text-align: center;">1st place</div></td>
            <td width="600px">
                <div style="text-align: center;">
                    <!-- <strong><span class="author">
                        Qian Wu<sup>1</sup>, Jun Liu<sup>1,2</sup><br>
                        from <sup>1</sup>Singapore University of Technology and Design and<br>
                        <sup>2</sup>Lancaster University
                    </span></strong> -->
                </div>
            </td>
        </tr>
        <tr class="row_type_A">
            <td width=50px"><div style="text-align: center;">2nd place</div></td>
            <td width="600px">
                <div style="text-align: center;">
                    <!-- <strong><span class="author">
                        Chengfeng Wang, Yuhang Yang, Wei Zhai,<br>
                        Yang Cao, Zhengjun Zha from<br>
                        University of Science and Technology of China (USTC)
                    </span></strong> -->
                </div>
            </td>
        </tr>
        <tr class="row_type_A">
            <td width=50px"><div style="text-align: center;">3rd place</div></td>
            <td width="600px">
                <div style="text-align: center;">
                    <!-- <strong><span class="author">
                        Seungeun Lee<sup>1</sup>, Donguk Kim<sup>2</sup>, Kyungmoon Lee<sup>3</sup><br>
                        from <sup>1</sup>KLLEON, <sup>2</sup>SHIFTUP, and <sup>3</sup>NALBI
                    </span></strong> -->
                </div>
            </td>
        </tr>

    </table>
    <br>
</center>

<h2 id="contact" style="text-align: center;">Contact Info</h2>
<p>E-mail: 
<a href="mailto:rhobinchallenge@gmail.com" target="_blank">rhobinchallenge@gmail.com</a>
</p>



<div style="clear: both;">&nbsp;</div>
</div><br><br>

</body>

</html>
