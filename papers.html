<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>2nd RHOBIN Workshop@CVPR24</title>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <link href="data/default.css" rel="stylesheet" type="text/css" />
    <link rel="icon" type="image/png" href="./data/rhobin-logo.png" />
    <meta property='og:title' content='RHOBIN Workshop: Reconstruction of Human-Object Interactions @ CVPR24'/>
    <meta property='og:url' content='https://rhobin-challenge.github.io/' />
    <meta property="og:type" content="website" />
    
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script>
        function moreOrLess(tableData) {
            obj = tableData.find("span");
            button = tableData.find("button");
            console.log(obj.hasClass("hidden"))
            if (obj.hasClass("hidden")) {
                obj.slideDown("fast");
                obj.removeClass("hidden");
                button.html("Hide Abstract");
            } else {
                obj.slideUp("fast");
                obj.addClass("hidden");
                button.html("Show Abstract");
            }
        }
    </script>
  </head>

  <body>
    <div id="header">
        <div id="logo">
          <h1>
              <center>
                  <span style="font-size:50%;color:#777;font-weight:normal">The third Workshop on</span><br>
                  Reconstruction of Human-Object Interactions (RHOBIN)
              </center>
          </h1><br>
          <div class="my-image-box-header">
              <center><img src="./data/rhobin-logo-no-bg.png" alt="rhobin-logo" width="128" height="168"></center>
          </div>
          <h2>
            <center>
                <span style="font-size:92%;color:#777;font-weight:normal">June 12, 08<sup>50</sup> - 12<sup>30</sup> @ Room 212, 
                    <a href="https://cvpr2025.thecvf.com/" target="_blank">CVPR 2025</a>, Nashville TN, USA &amp; <a href="https://cvpr.thecvf.com/virtual/2025/workshop/32335" target="_blank" style="text-decoration: underline;">online</a></span><br>
                <span style="font-size:130%;color:#777;font-weight:bold">Music City Center</span>
            </center>
          </h2><br>
      </div>
  
    <div id="menu">
    <center>
      <ul>
        <li class="first"><a href="./index.html" accesskey="1">Home</a></li>
      </ul>
    </center>
</div>

<button onclick="topFunction()" id="myBtn" title="Go to top">Top</button>

<script>
//Get the button
var mybutton = document.getElementById("myBtn");

// When the user scrolls down 20px from the top of the document, show the button
window.onscroll = function() {scrollFunction()};

function scrollFunction() {
      if (document.body.scrollTop > 20 || document.documentElement.scrollTop > 20) {
              mybutton.style.display = "block";
                } else {
                        mybutton.style.display = "none";
                          }
}

// When the user clicks on the button, scroll to the top of the document
function topFunction() {
      document.body.scrollTop = 0;
        document.documentElement.scrollTop = 0;
}
</script>
</div>


        <div id="content">
<!-- <p>
<ul>
<li>
    We are hosting <a href="https://app.gather.town/app/zDHzSsCHaRrEvEq4/ECCV%20VOLI%20Workshop" target="_blank">the poster session</a> on GatherTown (check out <a href="./gathertown_instruction.pdf" target="_blank">the GatherTown instructions</a> on how to join).
</li>
</ul>
</p> -->
<!-- <div style="margin-top: 100px;">
<br>
    <strong>
    <p class="poster_title">
    We are happy to announce the winners of the RHOBIN Challenge!
    Thanks to the generous sponsorhsip of <a href="https://www.adobe.com"></a><span style="font-size: 24px; color: #ff0000">Adobe
         </span></a>
    each winning team will receive a gift of 500 USD &#127881;!
</strong>
</div> -->

<center>
    <h2>Challenge Winners</h2><br>
    <table>
        <!--Human Reconstruction track-->
        <tr class="row_type_A">
            <td width=200px" class="paper_id" rowspan="2">Human Reconstruction track</td>
            <td width=50px"><div style="text-align: center;">1st place</div></td>
            <td width="600px">
                <div style="text-align: center;">
                    <!-- <strong><span class="author">
                        Fabien Baradel, Thomas Lucas, Matthieu Armando, Romain Brégier,<br>
                        Salma Galaaoui, Philippe Weinzaepfel, Gregory Rogez<br>
                        from NAVER LABS
                    </span></strong> -->
                </div>
            </td>
        </tr>
        <tr class="row_type_A">
            <td width=50px"><div style="text-align: center;">2nd place</div></td>
            <td width="600px">
                <div style="text-align: center;">
                    <!-- <strong><span class="author">
                        Yangxu Yan<sup>1</sup>, Anlong Ming<sup>1</sup>, Huadong Ma<sup>1</sup>, Zhaowen Lin<sup>1</sup>,<br>
                        Weihong Yao<sup>2</sup> from <br>
                        <sup>1</sup>Beijing University of Posts and Telecommunications and<br>
                        <sup>2</sup>Shanghai Vision Era Co. Ltd.
                    </span></strong> -->
                </div>
            </td>
        </tr>

        <!--Joint Human-Object Reconstruction track-->
        <tr class="row_type_B">
            <td width="200px" class="paper_id" rowspan="2">Joint Human-Object Reconstruction track</td>
            <td width=50px"><div style="text-align: center;">1st place</div></td>
            <td width="600px">
                <div style="text-align: center;">
                    <!-- <strong><span class="author">
                        Yuyang Jing<sup>1</sup>, Anlong Ming<sup>1</sup>, Yongchang Zhang<sup>1</sup>, Xiaohai Yu<sup>1</sup>,<br>
                        Huadong Ma<sup>1</sup>, Weihong Yao<sup>2</sup> from <br>
                        <sup>1</sup>Beijing University of Posts and Telecommunications and<br>
                        <sup>2</sup>Shanghai Vision Era Co. Ltd.
                    </span></strong> -->
                </div>
            </td>
        </tr>
        <tr class="row_type_B">
            <td width=50px"><div style="text-align: center;">2nd place</div></td>
            <td width="600px">
                <div style="text-align: center;">
                    <!-- <strong><span class="author">
                        Feng Xue<sup>1</sup>, Ke Han<sup>1</sup>, Tianrui Ye<sup>2</sup><br>
                        from <sup>1</sup>University of Trento and<br>
                        <sup>2</sup>University of North Carolina at Chapel Hill
                    </span></strong> -->
                </div>
            </td>
        </tr>

        <!--3D Contact Estimation track-->
        <tr class="row_type_A">
            <td width="200px" class="paper_id" rowspan="3">3D Contact Estimation track</td>
            <td width=50px"><div style="text-align: center;">1st place</div></td>
            <td width="600px">
                <div style="text-align: center;">
                    <!-- <strong><span class="author">
                        Qian Wu<sup>1</sup>, Jun Liu<sup>1,2</sup><br>
                        from <sup>1</sup>Singapore University of Technology and Design and<br>
                        <sup>2</sup>Lancaster University
                    </span></strong> -->
                </div>
            </td>
        </tr>
        <tr class="row_type_A">
            <td width=50px"><div style="text-align: center;">2nd place</div></td>
            <td width="600px">
                <div style="text-align: center;">
                    <!-- <strong><span class="author">
                        Chengfeng Wang, Yuhang Yang, Wei Zhai,<br>
                        Yang Cao, Zhengjun Zha from<br>
                        University of Science and Technology of China (USTC)
                    </span></strong> -->
                </div>
            </td>
        </tr>
        <tr class="row_type_A">
            <td width=50px"><div style="text-align: center;">3rd place</div></td>
            <td width="600px">
                <div style="text-align: center;">
                    <!-- <strong><span class="author">
                        Seungeun Lee<sup>1</sup>, Donguk Kim<sup>2</sup>, Kyungmoon Lee<sup>3</sup><br>
                        from <sup>1</sup>KLLEON, <sup>2</sup>SHIFTUP, and <sup>3</sup>NALBI
                    </span></strong> -->
                </div>
            </td>
        </tr>

    </table>
</center>

<div style="margin-top: 100px;">
<center>
    <h2>Paper Presentations</h2>
    <table>
        <tr class="row_type_A">
            <td width="20px" , class="paper_id">1</td>
            <td width="200px">
                <div style="text-align: center;">
                    <!-- <strong><span class="author">
                        Mara Levy <br>
                        (University of Maryland), <br>
                        Abhinav Shrivastava <br>
                        (University of Maryland)
                    </span></strong> -->
                </div>
                <!-- <div style="text-align: center;">
                    <a href="" target="_blank">[PDF]</a>
                    <a href="https://youtu.be/" target="_blank">[Video]</a>
                </div> -->
            </td>


            <td id="paper1">
                <!-- <p class="poster_title">
                    V-VIPE: Variational View Invariant Pose Embedding
                </p> -->
                <!-- <span class="hidden">
                    <p><strong>Abstract:</strong>
                        Learning to represent three dimensional (3D) human pose given a two dimensional (2D) image of a person, is a challenging problem. In order to make the problem less ambiguous it has become common practice to estimate 3D pose in the camera coordinate space. However, this makes the task of comparing two 3D poses difficult. In this paper, we address this challenge by separating the problem of estimating 3D pose from 2D images into two steps. We use a variational autoencoder (VAE) to find an embedding that represents 3D poses in world coordinate space using a canonical camera viewpoint. We refer to this embedding as variational view-invariant pose embedding V-VIPE. Using V-VIPE we can encode 2D and 3D poses and use the embedding for downstream tasks, like retrieval and classification. We can estimate 3D poses from these embeddings using the decoder as well as generate unseen 3D poses. The variability of our encoding allows it to generalize well to unseen camera views when mapping from 2D space. To the best of our knowledge, V-VIPE is the only representation to offer this diversity of applications.
                    </p>
                </span> -->
                <div style="text-align: center;">
                    <button onclick="moreOrLess($('#paper1'))">Show Abstract</button>
                </div>
            </td>
        </tr>

    <tr class="row_type_B">
            <td width="20px" , class="paper_id">2</td>
            <td width="200px">
                <div style="text-align: center;">
                    <!-- <strong><span class="author">
                        Bedirhan Uğuz <br>
                        (Middle East Technical University [METU]), <br>
                        Özhan Suat (METU), <br>
                        Batuhan Karagoz (METU), <br>
                        Emre Akbas (METU)
                    </span></strong> -->
                </div>
                <!-- <div style="text-align: center;">
                    <a href="" target="_blank">[PDF]</a>
                    <a href="https://youtu.be/" target="_blank">[Video]</a>
                </div> -->
            </td>
            <td id="paper2">
                <!-- <p class="poster_title">
                    MoCap-to-Visual Domain Adaptation for Efficient Human Mesh Estimation from 2D Keypoints
                </p>
                <span class="hidden">
                    <p><strong>Abstract:</strong>
                        This paper presents Key2Mesh, a model that takes a set of 2D human pose keypoints as input and estimates the corresponding body mesh. Since this process does not involve any visual (i.e. RGB image) data, the model can be trained on large-scale motion capture (MoCap) datasets, thereby overcoming the scarcity of image datasets with 3D labels. To enable the model's application on RGB images, we first run an off-the-shelf 2D pose estimator to obtain the 2D keypoints, and then feed these 2D keypoints to Key2Mesh. To improve the performance of our model on RGB images, we apply an adversarial domain adaptation (DA) method to bridge the gap between the MoCap and visual domains. Crucially, our DA method does not require 3D labels for visual data, which enables adaptation to target sets without the need for costly labels. We evaluate Key2Mesh for the task of estimating 3D human meshes from 2D keypoints, in the absence of RGB and mesh label pairs. Our results on widely used H3.6M and 3DPW datasets show that Key2Mesh sets the new state-of-the-art by outperforming other models in PA-MPJPE for both datasets, and in MPJPE and PVE for the 3DPW dataset. Thanks to our model's simple architecture, it operates at least 12x faster than the prior state-of-the-art model, LGD. We provide sample video outputs and additional qualitative samples in the supplementary material. Code will be released.
                    </p>
                </span> -->
                <div style="text-align: center;">
                    <button onclick="moreOrLess($('#paper2'))">Show Abstract</button>
                </div>
            </td>
        </tr>


        <tr class="row_type_B">
            <td width="20px" , class="paper_id">3</td>
            <td width="200px">
                <div style="text-align: center;">
                    <!-- <strong><span class="author">
                        Md Mushfiqur Azam <br>
                        (The University of Texas at San Antonio), <br>
                        Kevin Desai <br>
                        (University of Texas at San Antonio)
                    </span></strong> -->
                </div>
                <!-- <div style="text-align: center;">
                    <a href="" target="_blank">[PDF]</a>
                    <a href="https://youtu.be/" target="_blank">[Video]</a>
                </div> -->
            </td>
            <td id="paper3">
                <!-- <p class="poster_title">
                    A Survey on 3D Egocentric Human Pose Estimation
                </p>
                <span class="hidden">
                    <p><strong>Abstract:</strong>
                        Egocentric human pose estimation aims to estimate human body poses and develop body representations from a first-person camera perspective. It has gained vast popularity in recent years because of its wide range of applications in sectors like XR-technologies, human-computer interaction, and fitness tracking. However, to the best of our knowledge, there is no systematic literature review based on the proposed solutions regarding egocentric 3D human pose estimation. To that end, the aim of this survey paper is to provide an extensive overview of the current state of egocentric pose estimation research. In this paper, we categorize and discuss the popular datasets and the different pose estimation models, highlighting the strengths and weaknesses of different methods by comparative analysis. This survey can be a valuable resource for both researchers and practitioners in the field, offering insights into key concepts and cutting-edge solutions in egocentric pose estimation, its wide-ranging applications, as well as the open problems with future scope.
                    </p>
                </span>
                <div style="text-align: center;">
                    <button onclick="moreOrLess($('#paper3'))">Show Abstract</button>
                </div> -->
            </td>
        </tr>
    </table>
</center>
</div>

<!-- <center>
    <h2>Abstract Presentations</h2>
    <table>

        <tr class="row_type_B">

            <td width="200px">
                <div style="text-align: center;">
                    <strong><span class="author">
                        Aditya Prakash (University of Illinois Urbana-Champaign)*; Matthew Chang (UIUC); Matthew Jin (University of Illinois at Urbana-Champaign); Saurabh Gupta (UIUC)                         </div>
                <div style="text-align: center;">
                    <a href="" target="_blank">[PDF]</a>
                    <a href="https://youtu.be/" target="_blank">[Video]</a>
                </div>
            </td>
            <td id="abstract1">
                <p class="poster_title">
                    Learning Hand-Held Object Reconstruction from In-The-Wild Videos
                    <span class="hidden">
                    <p><strong>Abstract:</strong>
                        Prior works for reconstructing hand-held objects from a single image rely on direct 3D shape supervision which is challenging to gather in real world at scale. Consequently, these approaches do not generalize well when presented with novel objects in in-the-wild settings. While 3D supervision is a major bottleneck, there is an abundance of in-the-wild raw video data showing hand-object interactions. In this paper, we automatically extract 3D supervision (via multiview 2D supervision) from such raw video data (e.g. VISOR) to scale up the learning of models for hand-held object reconstruction. This requires tackling two key challenges: unknown camera pose and occlusion. For the former, we use hand pose (from existing techniques) as a proxy for object pose. For the latter, we learn data-driven 3D shape priors using synthetic objects from the ObMan dataset. We use these indirect 3D cues to train occupancy networks that predict the 3D shape of objects from a single RGB image. Our experiments on the MOW dataset show the effectiveness of these supervisory signals in predicting coherent 3D shapes and generalizing better to in-the-wild images compared to baselines. Since our approach does not require 3D supervision, we can also scale it up by incorporating additional datasets.
                    </p>
                </span>
                <div style="text-align: center;">
                </div>
            </td>
        </tr>


        <tr class="row_type_B">
            <td width="200px">
                <div style="text-align: center;">
                    <strong><span class="author">
                        Yi-Ling Qiao (University of Maryland, College Park)*; Shutong Zhang (University of Toronto); Guanglei Zhu (University of Toronto); Eric Heiden (NVIDIA); Dylan Turpin (University of Toronto); Ming C Lin (UMD-CP & UNC-CH ); Miles Macklin (NVIDIA); Animesh Garg (University of Toronto, Vector Institute, Nvidia)
                    </div>
                <div style="text-align: center;">
                    <a href="" target="_blank">[PDF]</a>
                    <a href="https://youtu.be/" target="_blank">[Video]</a>
                </div>
            </td>
            <td id="abstract2">
                <p class="poster_title">
                    HandyPriors: Physically Consistent Perception of Hand-Object Interactions with Differentiable Priors                         <span class="hidden">

                </span>
                <div style="text-align: center;">
                </div>
            </td>
        </tr>

    </table>
</center> -->

<h2 id="contact">Contact Info</h2>
<p>E-mail: 
<a href="mailto:rhobinchallenge@gmail.com" target="_blank">rhobinchallenge@gmail.com</a>
</p>



<div style="clear: both;">&nbsp;</div>
</div><br><br>

</body>

</html>
