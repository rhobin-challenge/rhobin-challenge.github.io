<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>3rd RHOBIN Workshop@CVPR25</title>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <link href="data/default.css" rel="stylesheet" type="text/css" />
    <link rel="icon" type="image/png" href="./data/rhobin-logo.png" />
    <meta property='og:title' content='RHOBIN Workshop: Reconstruction of Human-Object Interactions @ CVPR25'/>
    <meta property='og:url' content='https://rhobin-challenge.github.io/' />
    <meta property="og:type" content="website" />
  </head>

  <body>
    <div id="header">
      <div id="logo">
    <h1>
        <center>
             <span style="font-size:50%;color:#777;font-weight:normal">The third Workshop on</span><br>
             Reconstruction of Human-Object Interactions (RHOBIN)
        </center>
    </h1><br>
  <div class="my-image-box-header">
  <center>
    <img src="./data/rhobin-logo-no-bg.png" alt="rhobin-logo" width="128" height="168">
  </center>
    
  </div>
  <h2>
    <center>
      <span style="font-size:92%;color:#777;font-weight:normal">June 15, 13<sup>25</sup> - 18<sup>00</sup> @
          <a href="https://cvpr2025.thecvf.com/" target="_blank">CVPR 2025</a>, Nashville TN, USA</span><br>
      <span style="font-size:130%;color:#777;font-weight:bold">Music City Center</span>
    </center>
    </h2><br>
</div>

<div id="menu">
    <center>
      <ul>
        <li><a href="#speakers">Speakers</a></li>
        <li><a href="./schedule.html" accesskey="2">Schedule</a></li>
        <li><a href="./papers.html" accesskey="3">Papers & Winners</a></li>
        <li><a href="#contact">Contact</a></li>
        <li><a href="./index.html" accesskey="3">2<sup>nd</sup> RHOBIN @ CVPR'24</a></li>
      </ul>
    </center>
</div>


<button onclick="topFunction()" id="myBtn" title="Go to top">Top</button>

<script>
//Get the button
var mybutton = document.getElementById("myBtn");

// When the user scrolls down 20px from the top of the document, show the button
window.onscroll = function() {scrollFunction()};

function scrollFunction() {
      if (document.body.scrollTop > 20 || document.documentElement.scrollTop > 20) {
              mybutton.style.display = "block";
                } else {
                        mybutton.style.display = "none";
                          }
}

// When the user clicks on the button, scroll to the top of the document
function topFunction() {
      document.body.scrollTop = 0;
        document.documentElement.scrollTop = 0;
}
</script>


    <div id="content">
<h2>News</h2>
<p>
<ul>
  <li>
    [Feb. 7, 2025] Rhobin says hello CVPR'25!
  </li>
</ul>
</p>


<h2>Introduction</h2>
<!-- <p>
  Following the success of the first Rhobin workshop at CVPR'23, this second half-day Rhobin workshop 
  will continue providing a venue to present and discuss state-of-the-art research in the reconstruction 
  of human-object interactions from images. The focus of this second workshop will go beyond image-based 
  interaction reconstruction, extend to interaction tracking over time,  and seek connections to relevant 
  topics such as egocentric vision and dynamic scene interactions. The second Rhobin challenge will feature
  five tracks in total with two new tasks on human-object interaction tracking and image-based contact 
  estimation, using two new datasets InterCap and DAMON along with BEHAVE.  
</p> -->
<p>
  Following the success of the second Rhobin workshop at CVPR'23 and CVPR'24, this third half-day Rhobin workshop 
  will continue providing a venue to present and discuss state-of-the-art research in the reconstruction 
  of human-object interactions from images. The focus of this third workshop will go beyond image-based 
  interaction reconstruction, extend to interaction tracking over time, and seek connections to relevant 
  topics such as egocentric vision and dynamic scene interactions. The third Rhobin challenge will feature
  five tracks in total with tasks on human-object interaction tracking and image-based contact 
  estimation, using datasets InterCap, DAMON, and BEHAVE.
</p>
<p>
  Humans are in constant contact with the world as they move through it and interact with it. To better 
  understand how humans interact with the world, it is crucial to estimate human poses, shapes, and movements. 
  3D Human Pose and Motion estimation from images or videos has attracted a lot of interest. However, in most 
  cases, the task does not explicitly involve objects and the interaction with them. Whether it is 2D detection 
  and/or monocular 3D reconstruction, objects and humans have been mostly studied separately. Considering the 
  interaction between them can marry the best of both worlds. 
</p>

  <p>Participation details of the <b>Rhobin Challenge</b> and <b>paper submission</b> can be found below.
  </p>
    
<h2 id="speakers">Invited Speakers (Check out the <a href="./schedule.html">Full Schedule</a>)</h2>
<center>

<table style="width:100%">
<p>

<tr>
    <td colspan="2"><center><a href="https://fbogo.github.io" target="_blank"> <img alt src="https://fbogo.github.io/images/crop_small.jpg" height="170"/> </a></center> </td>
    <td colspan="2"><center><a href="https://people.eecs.berkeley.edu/~kanazawa" target="_blank"><img alt src="https://www2.eecs.berkeley.edu/Faculty/Photos/Homepages/kanazawa.jpg" height="170"/> </a></center> </td>
    <td colspan="2"><center><a href="https://ericyi.github.io" target="_blank"><img alt src="https://ericyi.github.io/Li_Yi_files/ericyi.jpg" height="170"/> </a></center></td>
</tr>
<tr>
    <td colspan="2"> <center> <h3> Federica Bogo </h3> </center></td>
    <td colspan="2"> <center> <h3> Angjoo Kanazawa </h3> </center></td>
    <td colspan="2"> <center> <h3> Eric Yi </h3> </center></td>
</tr>
<tr>
    <td colspan="2"> <center> <font size= "2">Meta</font></center> </td>
    <td colspan="2"> <center> <font size= "2">EECS, Berkeley<br>KAIR lab</font></center> </td>
    <td colspan="2"> <center> <font size= "2">Tsinghua University (IIIS)</font></center> </td>
</tr>

<tr>
    <td colspan="3"><center><a href="https://www.comp.nus.edu.sg/~ayao" target="_blank"> <img alt src="https://www.comp.nus.edu.sg/~ayao/files/profilephoto.png" height="170"/> </a></center> </td>
    <td colspan="3"><center><a href="https://jiajunwu.com" target="_blank"> <img alt src="https://jiajunwu.com/images/Jiajun_Wu.jpg" height="170"/> </a></center> </td>
</tr>
<tr> 
    <td colspan="3" > <center> <h3> Angela Yao </h3> </center></td>
    <td colspan="3" > <center> <h3> Jiajun_Wu </h3> </center></td>
</tr>
<tr>
    <td colspan="3"> <center> <font size= "2">National University of Singapore</font></center> </td>
    <td colspan="3"> <center> <font size= "2">Stanford University</font></center> </td>
</tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>

</p>
</table>
</center>

<!-- (Check out the <a href="./papers.html">Accepted Papers</a>) -->
<h2 id="cfp">Call for Papers </h2>
<p>
  In this workshop, we invite papers on topics related to human-centered interaction modeling.
  This could include, but is not limited to:
</p>
  <p>
      <ul>
        <li>Estimation of 3D human pose and shape from a single image or video</li>
        <li>3D human motion prediction</li>
        <li>Interactive motion sequence generation</li>
        <li>Shape reconstruction from a single image</li>
        <li>Object 6-DoF pose estimation and tracking</li>
        <li>Human-centered object semantics and functionality modeling</li>
        <li>Joint reconstruction of both bodies and objects/scenes</li>
        <li>Interaction modeling between humans and objects, e.g., contact, physics properties</li>
        <li>Detection of human-object interaction semantics</li>
        <li>New datasets or benchmarks that have 3D annotations of both humans and objects/scenes </li>
  </ul>
  </p>
  
<p>
  <p>We invite submissions of a maximum of 8 pages, excluding references, using the CVPR template. Submissions should follow CVPR 2025 instructions. All papers will be subject to a double-blind review process, i.e. authors must not identify themselves on the submitted papers. The reviewing process is single-stage without rebuttals.
    We also invite <b>1-page abstract</b> submissions of already published works or relevants works in progress.
  
    <h3>Submission Instructions</h3>
<br>
  Submissions are anonymous and should not include any author names, affiliations, and contact information in the PDF.
  <ul>
    <li>Online Submission System:  <a href="https://cmt3.research.microsoft.com/Rhobin2024/" target="_blank">https://cmt3.research.microsoft.com/</a></li>
    <!-- <li>Submission Format: <a href="https://github.com/cvpr-org/author-kit/releases/tag/CVPR2024-v2" target="_blank">official CVPR template</a>  (double column; no more than 8 pages, excluding reference).</li> -->

  </ul>
  If you have any questions, feel free to reach out to us.
  </p>
 
  <h3>Timeline Table (11:59 PM, Pacific Time)</h3>
  <ul>
      <li>Full-paper submission deadline: March 20, 2025</li>
      <li>Notification to authors: April 3, 2025</li>
      <li>1-page Abstract submission deadline: May 15, 2025</li>
      <li>Camera-ready deadline: April 14, 2025</li>
      <li>Workshop: June 15 PM, 2025</li>
  </ul>
</p>

  <!-- 
UPDATE WITH CVPR 2025
  <h3>Poster Presentation (June 17 3:15-4:00 PM, Pacific Time)</h3>
<br>
We will provide you with a poster board in our workshop seminar room. The workshop lasts for half a day and you can hang your poster there during that time. We will also have a 45-minute poster session at 3:15-4:00pm PST.
We use the same size as the CVPR poster hence you can also use the same poster template here:
https://media.eventhosts.cc/Conferences/CVPR2024/cvpr24_poster_template.pptx
You can find more information regarding workshop posters here (under Workshop Posters):
https://cvpr.thecvf.com/Conferences/2024/PosterPrintingInformation 

If you encounter any problems, please don't hesitate to contact us at rhobinchallenge@gmail.com.  -->

<h2 id="cfp">The Second Rhobin Challenge</h2>
<p>
  Given the importance of human-object interaction, as also highlighted by this workshop, we 
  propose a challenge on reconstructing 3D human and object and estimating 3D human-object 
  and human-scene contact, from monocular RGB images. We have seen promising progress in 
  reconstructing human body mesh or estimating 6DoF object pose from single images. However, 
  most of these works focus on occlusion-free images which are not realistic for settings 
  during close human-object interaction since humans and objects occlude each other. This 
  makes inference more difficult and poses challenges to existing state-of-the-art methods. 
  Similarly, methods estimating 3D contacts have also seen rapid progress, but are restricted 
  to scanned or synthetic datasets, and struggle with generalization to in-the-wild scenarios.  
  In this workshop, we want to examine how well the existing human and object reconstruction 
  and contact estimation methods work under more realistic settings and more importantly, 
  understand how they can benefit each other for accurate interaction reasoning. The recently 
  released BEHAVE (CVPR'22), InterCap (GCPR’22) and DAMON (ICCV’23) datasets enable joint 
  reasoning about human-object interactions in real settings and evaluating contact prediction 
  in the wild. 
</p>
<h3>Challenge website</h3>
  <ul>
    <li><a href="https://codalab.lisn.upsaclay.fr/competitions/17571" target="_blank">3D human reconstruction</a></li>
    <li><a href="https://codalab.lisn.upsaclay.fr/competitions/17524" target="_blank">6DoF pose estimation of rigid objects</a></li>
    <li><a href="https://codalab.lisn.upsaclay.fr/competitions/17522" target="_blank">Joint reconstruction of human and object</a></li>
    <li><a href="https://codalab.lisn.upsaclay.fr/competitions/17572" target="_blank">Video-based tracking</a></li>
    <li><a href="https://codalab.lisn.upsaclay.fr/competitions/17561" target="_blank">3D contact prediction from 2D images</a></li>
  </ul>
  
  <h3>Important dates</h3>
  <ul>
      <li>Challenge open: <b>February 5 00:00, 2025 UTC</b></li>
      <li>Submission deadline: <b>May 30 23:59, 2025 UTC</b></li>
      <li>Winner award: June 15, 2025</li>
  </ul>
  

<h2 id="organizers">Workshop Organizers</h2>
<center>

<table style="width:100%">
<p>

<tr>
  <td colspan="2"><center><a href="https://xiwang1212.github.io/homepage/" target="_blank"> <img alt src="data/xiwang.jpg" height="170"/> </a></center> </td>
  <td colspan="2"><center><a href="https://people.mpi-inf.mpg.de/~xxie/" target="_blank"> <img alt src="data/xianghui.png" height="170"/> </a></center> </td>
  <td colspan="2"><center><a href="https://is.mpg.de/~nathanasiou" target="_blank"> <img alt src="data/nathanasiou.jpg" height="170"/> </a></center> </td>
</tr>
<tr>
  <td colspan="2"> <center> <h3> Xi Wang </h3> </center></td>
  <td colspan="2"> <center> <h3> Xianghui Xie </h3> </center></td>
  <td colspan="2"> <center> <h3> Nikos Athanasiou </h3> </center></td>
</tr>
<tr>
  <td colspan="2"> <center> <font size= "2"> ETH, Zurich</font></center> </td>
  <td colspan="2"> <center> <font size= "2"> Max Planck Institute for Informatics, Saarland Informatics Campus</font></center> </td>
  <td colspan="2"> <center> <font size= "2"> Max-Planck-Institute for Intelligen Systems, Tübingen</font></center> </td>
</tr>

<tr>
  <td  colspan="2"><center><a href="https://cs.stanford.edu/~kaichun/" target="_blank"><img alt src="data/kaichun.jpg" height="170"/> </a></center> </td>
  <td colspan="2"><center><a href="https://scholar.google.co.uk/citations?user=pZPD0hMAAAAJ&hl=en" target="_blank"> <img alt src="data/julien_valentin.jpg" height="170"/> </a></center> </td>
  <td colspan="2"><center><a href="https://dtzionas.com/" target="_blank"> <img alt src="data/dmitrios_tzionas.jpg" height="170"/> </a></center> </td>
</tr>
<tr>
  <td colspan="2"> <center> <h3> Kaichun Mo </h3> </center></td>
  <td colspan="2"> <center> <h3> Julien Valentin </h3> </center></td>
  <td colspan="2"> <center> <h3> Dimitrios Tzionas </h3> </center></td>
</tr>
<tr>
  <td colspan="2"> <center> <font size= "2">NVIDIA Research, Seattle</font></center> </td>
  <td colspan="2"> <center> <font size= "2"> Microsoft </font></center> </td>
  <td colspan="2"> <center> <font size= "2"> University of Amsterdam </font></center> </td>
</tr>

<tr>
  <td colspan="2"><center><a href="https://ait.ethz.ch/people/hilliges/" target="_blank"> <img alt src="data/otmarhilliges.jpg" height="170"/> </a></center> </td>
  <td colspan="2"><center><a href="https://ee.ethz.ch/the-department/faculty/professors/person-detail.OTAyMzM=.TGlzdC80MTEsMTA1ODA0MjU5.html" target="_blank"><img alt src="data/luc_van_gool.jpeg" height="170"/> </a></center> </td>
  <td colspan="2"><center><a href="https://virtualhumans.mpi-inf.mpg.de/people/pons-moll.html" target="_blank"> <img alt src="data/GPM_new_crop.png" height="170"/> </a></center> </td>
</tr>
<tr>
  <td colspan="2"> <center> <h3> Otmar Hilliges  </h3> </center></td>
  <td colspan="2"> <center> <h3> Luc Van Gool </h3> </center></td>
  <td colspan="2"> <center> <h3> Gerard Pons-Moll</h3> </center></td>
</tr>
<tr>
  <td colspan="2"> <center> <font size= "2"> ETH, Zurich</font></center> </td>
  <td colspan="2"> <center> <font size= "2"> ETH, Zurich</font></center> </td>
  <td colspan="2"> <center> <font size= "2"> University of Tübingen <br>Max Planck Institute for Informatics, Saarland Informatics Campus</font></center> </td>
</tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>

</p>
</table>
</center>

<h2 id="organizers">Challenge Organizers</h2>
<center>

<table style="width:100%">
<p>

<tr>
  <td><center><a href="https://people.mpi-inf.mpg.de/~xxie/" target="_blank"> <img alt src="data/xianghui.png" height="170"/> </a></center> </td>
  <td><center><a href="https://sha2nkt.github.io/" target="_blank"> <img alt src="data/shashank.jpg" height="170"/> </a></center> </td>
  <td><center><a href="https://virtualhumans.mpi-inf.mpg.de/people/Petrov.html" target="_blank"> <img alt src="data/ilya.jpeg" height="170"/> </a></center> </td>
</tr>
<tr>
  <td> <center> <h3> Xianghui Xie </h3> </center></td>
  <td> <center> <h3> Shashank Tripathi </h3> </center></td>
  <td> <center> <h3> Ilya A. Petrov </h3> </center></td>
</tr>
<tr>
  <td> <center> <font size= "2">Max Planck Institute for Informatics, Saarland Informatics Campus </font></center> </td>
  <td> <center> <font size= "2">Max-Planck-Institute for Intelligen Systems, Tübingen </font></center> </td>
  <td> <center> <font size= "2"> University of Tübingen </font></center> </td>
</tr>

<tr>
  <td><center><a href="https://virtualhumans.mpi-inf.mpg.de/people/Bhatnagar.html" target="_blank"> <img alt src="data/bhatnagar.jpg" height="170"/> </a></center> </td>
  <td><center><a href="https://dtzionas.com/" target="_blank"> <img alt src="data/dmitrios_tzionas.jpg" height="170"/> </a></center> </td>
  <td><center><a href="https://virtualhumans.mpi-inf.mpg.de/people/pons-moll.html" target="_blank"> <img alt src="data/GPM_new_crop.png" height="170"/> </a></center> </td>
</tr>
<tr>
  <td> <center> <h3> Bharat Lal Bhatnagar </h3> </center></td>
  <td> <center> <h3> Dimitrios Tzionas </h3> </center></td>
  <td> <center> <h3> Gerard Pons-Moll </h3> </center></td>
</tr>
<tr>
  <td> <center> <font size= "2"> Meta Reality Labs </font></center> </td>
  <td> <center> <font size= "2"> University of Amsterdam </font></center> </td>
  <td> <center> <font size= "2"> University of Tübingen <br>Max Planck Institute for Informatics, Saarland Informatics Campus</font></center> </td>
</tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>


</p>
</table>
</center>

<h2 id="Committee Members">Committee Members</h2>
<p>
  Bharat Lal Bhatnagar, Meta, Switzerland <br>>
  Dimitrios Tzionas, University of Amsterdam, Netherlands <br>
  Gerard Pons-Moll, University of Tübingen and MPI, Germany <br>
  Ilya A. Petrov, University of Tübingen, Germany <br>
  Julien Valentin, Microsoft, Switzerland <br>
  Kaichun Mo, NVIDIA, USA <br>
  Luc Van Gool, ETH Zurich, Switzerland <br>
  Nikos Athanasiou, MPI Intelligent System, Germany <br>
  Otmar Hilliges, ETH Zurich, Switzerland <br>
  Rongyu Chen, National University of Singapore, Singapore <br> 
  Shashank Tripathi, MPI Intelligent System, Germany <br>
  Xianghui Xie, MPI, Germany  <br>
  Xiaohan Zhang, University of Tübingen, Germany <br>
  Xi Wang, ETH Zurich, Switzerland <br>
  Yuxuan Xue, University of Tübingen, Germany <br>
  Zicong Fan, ETH Zurich, Switzerland <br>
</p>

<h2 id="contact">Contact Info</h2>
<p>E-mail: 
<a href="mailto:rhobinchallenge@gmail.com" target="_blank">rhobinchallenge@gmail.com</a>
</p>

<h2 id="sponsors">Sponsors</h2>
<p>
    <!-- ADD AND CHANGE SPONSORS HERE
    The Second RHOBIN Challenge has been generously supported by:
    <center>
    <table style="width:100%">
        <tr>
            <td> <center> <img alt src="data/meshcapade.png" height="120"/> </center></td>
            <td> <center> <img alt src="data/spreeai.png" height="90"/> </center></td>
            <td> <center> <img alt src="data/Apple_Logo_Black.png" height="170"/> </center></td>
        </tr>
    </table>
    </center> -->
</p>

<h2 id="acknowledgements">Acknowledgements</h2>
<p>Website template borrowed from: 
<a href="https://futurecv.github.io/" target="_blank">https://futurecv.github.io/</a>
(Thanks to <a href="https://www.cs.cmu.edu/~dpathak/" target="_blank">Deepak Pathak</a>)
</p>


<div style="clear: both;">&nbsp;</div>
</div><br><br>

</body>
</html>
