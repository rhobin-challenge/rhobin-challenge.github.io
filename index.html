<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>CVPR23 RHOBIN Workshop</title>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <link href="data/default.css" rel="stylesheet" type="text/css" />
    <meta property='og:title' content='ECCV22 RHOBIN Workshop: Reconstruction of Human-Object Intereactions'/>
    <meta property='og:url' content='https://rhobin-challenge.github.io/' />
    <meta property="og:type" content="website" />
  </head>

  <body>
    <div id="header">
      <div id="logo">
	<h1>
		<center>
	    	 <span style="font-size:50%;color:#777;font-weight:normal">The first Workshop on</span><br>
             Visual Object-oriented Learning meets Interaction (VOLI): <br>
             Discovery, Representations, and Applications
		</center>
	</h1><br>
	<h2>
		<center>
      <span style="font-size:92%;color:#777;font-weight:normal">October 24 @ 
          <a href="https://eccv2022.ecva.net/" target="_blank">ECCV 2022</a> 
      <span style="font-size:92%;color:#777;font-weight:bold">Virtual</span>
		</center>
	</h2><br>
	</div>

  <div id="menu">
    <center>
      <ul>
        <li><a href="#speakers">Speakers</a></li>
        <li><a href="./schedule.html" accesskey="2">Schedule</a></li>
        <li><a href="./papers.html" accesskey="3">Papers</a></li>
        <li><a href="#contact">Contact</a></li>
      </ul>
    </center>
</div>


<button onclick="topFunction()" id="myBtn" title="Go to top">Top</button>

<script>
//Get the button
var mybutton = document.getElementById("myBtn");

// When the user scrolls down 20px from the top of the document, show the button
window.onscroll = function() {scrollFunction()};

function scrollFunction() {
      if (document.body.scrollTop > 20 || document.documentElement.scrollTop > 20) {
              mybutton.style.display = "block";
                } else {
                        mybutton.style.display = "none";
                          }
}

// When the user clicks on the button, scroll to the top of the document
function topFunction() {
      document.body.scrollTop = 0;
        document.documentElement.scrollTop = 0;
}
</script>


	<div id="content">
<h2>News</h2>
<p>
<ul>
<li>
    [Jan 16, 2023] Workshop website launched, with <a href="#cfp">Call-for-Papers</a> and <a href="#speakers">speakers</a> announced.
</li>
</ul>
</p>


<h2>Introduction</h2>
<p>
This half-day workshop will provide a venue to present and discuss state-of-the-art research in
the reconstruction of human-object interactions from images. The focus will be on recent
developments in human-object interaction learning and its impact on 3D scene parsing, building
human-centric robotic assistants, and the general understanding of human behaviors.
</p>
<p>
Humans are an essential component of the interaction. Hence, it is crucial to estimate the
human pose, shape, and motion as well as objects that are being interacted with accurately to
achieve a realistic interaction. 3D Human Pose and Motion estimation from images or videos
have attracted a lot of interest. However, in most cases, the task does not explicitly involve
objects and the interaction with them. Whether it is 2D detection and/or monocular 3D
reconstruction, objects and humans have been studied separately. Humans are in constant
contact with the world as they move through it and interact with it. Considering the interaction
between them can marry the best of both worlds.
</p>
<p>
In this workshop, we invite papers on topics related to human-centered interaction modeling.
This could include, but is not limited to:
</p>
<p>
    <ul>
      <li>Estimation of 3D human pose and shape from a single image or video</li>
      <li>3D human motion prediction</li>
      <li>Interactive motion sequence generation</li>
      <li>Shape reconstruction from a single image</li>
      <li>Object 6-DoF pose estimation and tracking</li>
      <li>Human-centered object semantics and functionality modeling</li>
      <li>Joint reconstruction of both bodies and objects/scenes</li>
      <li>Interaction modeling between humans and objects, e.g., contact, physics properties</li>
      <li>Detection of human-object interaction semantics</li>
      <li>New datasets or benchmarks that have 3D annotations of both humans and objects/scenes </li>
</ul>
</p>

	
<h2 id="speakers">Invited Speakers (Check out the <a href="./schedule.html">Full Schedule</a>)</h2>
<center>

<table style="width:100%">
<p>

<tr>
<td><center><a href="http://people.ciirc.cvut.cz/~sivic/" target="_blank"> <img alt src="data/josef_sivic.jpg" height="170"/> </a></center> </td>
<td><center><a href="https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html" target="_blank"><img alt src="data/siyu.jpeg" height="170"/> </a></center> </td>
<td><center><a href="https://tsattler.github.io/" target="_blank"><img alt src="data/Torsten_profile_small.jpg" height="170"/> </a></center></td>
<td><center><a href="https://www.cs.utexas.edu/users/grauman/" target="_blank"> <img alt src="data/grauman.jpg" height="170"/> </a></center> </td>
</tr>
<tr>
<td> <center> <h3> Josef Sivick</h3> </center></td>
<td> <center> <h3> Prof. Siyu Tang</h3> </center></td>
<td> <center> <h3> Torsten Sattler</h3> </center></td>
<td> <center> <h3> Kristen Grauman </h3> </center></td>
</tr>
<tr>
<td> <center> <font size= "2">Czech Institute of Informatics, Robotics and Cybernetics (CIIRC) at the Czech Technical University (CTU)</font></center> </td>
<td> <center> <font size= "2">ETH ZÃ¼rich</font></center> </td>
<td> <center> <font size= "2">Czech Institute of Informatics, Robotics and Cybernetics (CIIRC) at the Czech Technical University (CTU)</font></center> </td>
<td> <center> <font size= "2">University of Texas Austin,Facebook AI Research (FAIR)</font></center> </td>
</tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>

</p>
</table>
</center>


<h2 id="cfp">Call for Papers (Check out the <a href="./papers.html">Accepted Papers</a>)</h2>
<p>
We accept both archival and non-archival paper submissions. The accepted archival papers will be included in the ECCV22 conference proceedings, while the non-archival ones will just be presented in the workshop. All papers will be peer-reviewed by three experts in the field in a double-blind manner. We also welcome papers that are accepted to the ECCV main conference or other previous conferences to present your work in the non-archival paper track. There is no need for peer-review for such previously accepted papers, so please indicate clearly in the submission form. Every accepted paper will have the opportunity to give a 5-min spotlight presentation and host two 30-min poster sessions (12-hours separated).
</p>
<h3>Submission Site: <a href="https://cmt3.research.microsoft.com/" target="_blank">https://cmt3.research.microsoft.com/</a></h3><br>
<h3>Submission Instructions:</h3>
<p>Please use the <a href="https://cvpr2023.thecvf.com/Conferences/2023/CallForPapers" target="_blank">official CVPR template</a> (under "Author Guidelines") for your submissions. Make sure to anonymize your submission.</p>
<h3>Timeline Table (11:59 PM, Pacific Time)</h3>
<ul>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
</ul>
</p>



<h2 id="organizers">Organizers</h2>
<center>

<table style="width:100%">
<p>

<tr>
<td><center><a href="https://xiwang1212.github.io/homepage/" target="_blank"> <img alt src="data/kaichun.jpg" height="170"/> </a></center> </td>
<td><center><a href="https://cs.stanford.edu/~kaichun/" target="_blank"><img alt src="data/yanchao.jpg" height="170"/> </a></center> </td>
<td><center><a href="https://virtualhumans.mpi-inf.mpg.de/people/Bhatnagar.html" target="_blank"> <img alt src="data/jiayuan.jpg" height="170"/> </a></center> </td>
</tr>
<tr>
<td> <center> <h3> Xi Wang </h3> </center></td>
<td> <center> <h3> Kaichun Mo </h3> </center></td>
<td> <center> <h3> Bharat Lal Bhatnagar </h3> </center></td>
</tr>
<tr>
<td> <center> <font size= "2">ETH</font></center> </td>
<td> <center> <font size= "2">NVIDIA Research</font></center> </td>
<td> <center> <font size= "2"> Max-Planck-Institute for Informatics</font></center> </td>
</tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>

<tr>
<td><center><a href="https://is.mpg.de/~nathanasiou" target="_blank"> <img alt src="data/shubham.jpg" height="170"/> </a></center> </td>
<td><center><a href="https://people.mpi-inf.mpg.de/~xxie/" target="_blank"> <img alt src="data/hongjing.jpg" height="170"/> </a></center> </td>
<td><center><a href="https://research.adobe.com/person/paulchhuang/" target="_blank"> <img alt src="data/leo.jpg" height="170"/> </a></center> </td>
</tr>
<tr>
<td> <center> <h3> Nikos Athanasiou </h3> </center></td>
<td> <center> <h3> Xianghui Xie</h3> </center></td>
<td> <center> <h3>  Chun-Hao (Paul) Huang  </h3> </center></td>
</tr>
<tr>
<td> <center> <font size= "2"> Max-Planck Institute for Intelligent Systems </font></center> </td>
<td> <center> <font size= "2">  Max-Planck-Institute for Informatics </font></center> </td>
<td> <center> <font size= "2"> Adobe, London </font></center> </td>
</tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>

</p>
</table>
</center>


<h2 id="contact">Contact Info</h2>
<p>E-mail: 
<a href="mailto:" target="_blank">kaichun@cs.stanford.edu</a>
</p>

<h2 id="contact">Acknowledgements</h2>
<p>Website template borrowed from: 
<a href="https://futurecv.github.io/" target="_blank">https://futurecv.github.io/</a>
(Thanks to <a href="https://www.cs.cmu.edu/~dpathak/" target="_blank">Deepak Pathak</a>)
</p>


<div style="clear: both;">&nbsp;</div>
</div><br><br>

</body>
</html>
