<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>3rd RHOBIN Workshop@CVPR25</title>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <link href="data/default.css" rel="stylesheet" type="text/css" />
    <link rel="icon" type="image/png" href="./data/rhobin-logo.png" />
    <meta property='og:title' content='RHOBIN Workshop: Reconstruction of Human-Object Interactions @ CVPR25'/>
    <meta property='og:url' content='https://rhobin-challenge.github.io/' />
    <meta property="og:type" content="website" />
  </head>

  <body>
    <div id="header">
      <div id="logo">
    <h1>
        <center>
             <span style="font-size:50%;color:#777;font-weight:normal">The third Workshop on</span><br>
             Reconstruction of Human-Object Interactions (RHOBIN)
        </center>
    </h1><br>
  <div class="my-image-box-header">
  <center>
    <img src="./data/rhobin-logo-no-bg.png" alt="rhobin-logo" width="128" height="168">
  </center>
    
  </div>
  <h2>
    <center>
      <span style="font-size:92%;color:#777;font-weight:normal">June 12, 08<sup>50</sup> - 12<sup>30</sup> @
          <a href="https://cvpr2025.thecvf.com/" target="_blank">CVPR 2025</a>, Nashville TN, USA</span><br>
      <span style="font-size:130%;color:#777;font-weight:bold">Music City Center</span>
    </center>
    </h2><br>
</div>

<div id="menu">
    <center>
      <ul>
        <li><a href="#speakers">Speakers</a></li>
        <li><a href="./schedule.html" accesskey="2">Schedule</a></li>
        <li><a href="./papers.html" accesskey="3">Papers & Winners</a></li>
        <li><a href="#contact">Contact</a></li>
        <li><a href="https://cvpr.thecvf.com/virtual/2024/workshop/23605" accesskey="3">2<sup>nd</sup> RHOBIN @ CVPR'24</a></li>
      </ul>
    </center>
</div>


<button onclick="topFunction()" id="myBtn" title="Go to top">Top</button>

<script>
//Get the button
var mybutton = document.getElementById("myBtn");

// When the user scrolls down 20px from the top of the document, show the button
window.onscroll = function() {scrollFunction()};

function scrollFunction() {
      if (document.body.scrollTop > 20 || document.documentElement.scrollTop > 20) {
              mybutton.style.display = "block";
                } else {
                        mybutton.style.display = "none";
                          }
}

// When the user clicks on the button, scroll to the top of the document
function topFunction() {
      document.body.scrollTop = 0;
        document.documentElement.scrollTop = 0;
}
</script>


    <div id="content">
<h2>News</h2>
<p>
<ul>
  <li>
    [Feb. 7, 2025] Rhobin says hello CVPR'25!
  </li>
</ul>
</p>


<h2>Introduction</h2>
<!-- <p>
  Following the success of the first Rhobin workshop at CVPR'23, this second half-day Rhobin workshop 
  will continue providing a venue to present and discuss state-of-the-art research in the reconstruction 
  of human-object interactions from images. The focus of this second workshop will go beyond image-based 
  interaction reconstruction, extend to interaction tracking over time,  and seek connections to relevant 
  topics such as egocentric vision and dynamic scene interactions. The second Rhobin challenge will feature
  five tracks in total with two new tasks on human-object interaction tracking and image-based contact 
  estimation, using two new datasets InterCap and DAMON along with BEHAVE.  
</p> -->
<p>
  Following the success of the first and second Rhobin workshop at CVPR'23 and CVPR'24, this third half-day Rhobin workshop 
  will continue providing a venue to present and discuss state-of-the-art research in the reconstruction 
  of human-object interactions from images. The focus of this third workshop will go beyond image-based 
  interaction reconstruction, extend to interaction tracking over time, and seek connections to relevant 
  topics such as egocentric vision and dynamic scene interactions. The third Rhobin challenge will feature
  five tracks in total with tasks on human-object interaction tracking and image-based contact 
  estimation, using datasets InterCap, DAMON, and BEHAVE.
</p>
<p>
  Humans are in constant contact with the world as they move through it and interact with it. To better 
  understand how humans interact with the world, it is crucial to estimate human poses, shapes, and movements. 
  3D Human Pose and Motion estimation from images or videos has attracted a lot of interest. However, in most 
  cases, the task does not explicitly involve objects and the interaction with them. Whether it is 2D detection 
  and/or monocular 3D reconstruction, objects and humans have been mostly studied separately. Considering the 
  interaction between them can marry the best of both worlds. 
</p>

  <p>Participation details of the <b>Rhobin Challenge</b> and <b>paper submission</b> can be found below.
  </p>
    
<h2 id="speakers">Invited Speakers (Check out the <a href="./schedule.html">Full Schedule</a>)</h2>
<center>

<table style="width:100%">
<p>

<tr>
    <td><center><a href="https://fbogo.github.io" target="_blank"> <img alt src="data/federica_bogo.jpg" height="170"/> </a></center> </td>
    <td><center><a href="https://ericyi.github.io" target="_blank"><img alt src="data/eric_yi.jpg" height="170"/> </a></center></td>
    <td><center><a href="https://www.comp.nus.edu.sg/~ayao" target="_blank"> <img alt src="data/angela_yao.png" height="170"/> </a></center> </td>
    <td><center><a href="https://jiajunwu.com" target="_blank"> <img alt src="data/jiajun_wu.jpg" height="170"/> </a></center> </td>
</tr>
<tr>
    <td> <center> <h3> Federica Bogo </h3> </center></td>
    <td> <center> <h3> Eric Yi </h3> </center></td>
    <td> <center> <h3> Angela Yao </h3> </center></td>
    <td> <center> <h3> Jiajun Wu </h3> </center></td>
</tr>
<tr>
    <td> <center> <font size= "2">Meta</font></center> </td>
    <td> <center> <font size= "2">Tsinghua University (IIIS)</font></center> </td>
    <td> <center> <font size= "2">National University of Singapore</font></center> </td>
    <td> <center> <font size= "2">Stanford University</font></center> </td>
</tr>

<!--
<tr>
</tr>
<tr> 
</tr>
<tr>
</tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>
-->

</p>
</table>
</center>

<!-- (Check out the <a href="./papers.html">Accepted Papers</a>) -->
<h2 id="cfp">Call for Papers </h2>
<p>
  In this workshop, we invite papers on topics related to human-centered interaction modeling.
  This could include, but is not limited to:
</p>
  <p>
      <ul>
        <li>Estimation of 3D human pose and shape from images or video</li>
        <li>3D human motion prediction</li>
        <li>Interactive motion sequence generation</li>
        <li>Interactive motion sequence generation/synthesis</li>
        <li>Shape reconstruction from a single image</li>
        <li>Object 6-DoF pose estimation and tracking</li>
        <li>Human-centered object semantics and functionality modeling</li>
        <li>Joint reconstruction of bodies and objects/scenes</li>
        <li>Interaction modeling between humans and objects, e.g., contact, physics properties</li>
        <li>Detection of human-object interaction semantics</li>
        <li>New datasets or benchmarks that have 3D annotations of both humans and objects/scenes</li>
        <li>Detection of holistic and/or semantic contact and downstream applications</li>
        <li>Estimation and modeling of physics and biomechanical properties</li>
        <li>Tracking human-object interactions in 2D/3D</li>
        <li>Template free interaction reconstruction</li>
        <li>Semantic 3D contact estimation</li>
  </ul>
  </p>
  
<p>
  <p>We invite submissions of a maximum of 8 pages, excluding references, using the CVPR template. Submissions should follow CVPR 2025 instructions. All papers will be subject to a double-blind review process, i.e. authors must not identify themselves on the submitted papers. The reviewing process is single-stage without rebuttals.
    We also invite <b>1-page abstract</b> submissions of already published works or relevants works in progress.
  
    <h3>Submission Instructions</h3>
<br>
  Submissions are anonymous and should not include any author names, affiliations, and contact information in the PDF.
  <ul>
    <li>Online Submission System: <a href="https://openreview.net/group?id=thecvf.com/CVPR/2025/Workshop/RHOBIN" target="_blank">https://openreview.net/group?id=thecvf.com/CVPR/2025/Workshop/RHOBIN</a></li>
    <!-- <li>Submission Format: <a href="https://github.com/cvpr-org/author-kit/releases/tag/CVPR2024-v2" target="_blank">official CVPR template</a>  (double column; no more than 8 pages, excluding reference).</li> -->

  </ul>
  If you have any questions, feel free to reach out to us.
  </p>
 
  <h3>Timeline Table (11:59 PM, Pacific Time)</h3>
  <ul>
      <li>Submissions open: February 20, 2025</li>
      <li>Submission deadline: March 17, 2025</li>
      <li>Notification to authors: March 31, 2025</li>
      <li>Camera-ready deadline: April 7, 2025</li>
      <!--
      <li>Full-paper submission deadline: March 20, 2025</li>
      <li>Notification to authors: April 3, 2025</li>
      <li>1-page Abstract submission deadline: May 15, 2025</li>
      <li>Camera-ready deadline: April 14, 2025</li>
      <li>Workshop: June 15 PM, 2025</li>
      -->
  </ul>
</p>

  <!-- 
UPDATE WITH CVPR 2025
  <h3>Poster Presentation (June 17 3:15-4:00 PM, Pacific Time)</h3>
<br>
We will provide you with a poster board in our workshop seminar room. The workshop lasts for half a day and you can hang your poster there during that time. We will also have a 45-minute poster session at 3:15-4:00pm PST.
We use the same size as the CVPR poster hence you can also use the same poster template here:
https://media.eventhosts.cc/Conferences/CVPR2024/cvpr24_poster_template.pptx
You can find more information regarding workshop posters here (under Workshop Posters):
https://cvpr.thecvf.com/Conferences/2024/PosterPrintingInformation 

If you encounter any problems, please don't hesitate to contact us at rhobinchallenge@gmail.com.  -->

<h2 id="cfp">The Third Rhobin Challenge</h2>
<p>
  Given the importance of human-object interaction, as also highlighted by this workshop, we 
  propose a challenge on reconstructing 3D human and object and estimating 3D human-object 
  and human-scene contact, from monocular RGB images. We have seen promising progress in 
  reconstructing human body meshes or estimating 6DoF object pose from single images. However, 
  most of these works focus on occlusion-free images which are not realistic for settings 
  during close human-object interaction since humans and objects occlude each other. This 
  makes inference more difficult and poses challenges to existing state-of-the-art methods. 
  Similarly, methods estimating 3D contacts have also seen rapid progress, but are restricted 
  to scanned or synthetic datasets, and struggle with generalization to in-the-wild scenarios.  
  In this workshop, we want to examine how well the existing human and object reconstruction 
  and contact estimation methods work under more realistic settings and more importantly, 
  understand how they can benefit each other for accurate interaction reasoning. The recently 
  released BEHAVE (CVPR'22), InterCap (GCPR’22) and DAMON (ICCV’23) datasets enable joint 
  reasoning about human-object interactions in real settings and evaluating contact prediction 
  in the wild. 
</p>
<h3>Challenge website</h3>
  <ul>
    <li><a href="https://codalab.lisn.upsaclay.fr/competitions/21687" target="_blank">3D human reconstruction track</a></li>
    <li><a href="https://codalab.lisn.upsaclay.fr/competitions/21755" target="_blank">Object 6DoF pose estimation track</a></li>
    <li><a href="https://codalab.lisn.upsaclay.fr/competitions/21752" target="_blank">Joint human object reconstruction</a></li>
    <li><a href="https://codalab.lisn.upsaclay.fr/competitions/21680" target="_blank">Template-free interaction reconstruction</a></li>
    <li><a href="https://codalab.lisn.upsaclay.fr/competitions/21697" target="_blank">Tracking human-object interaction</a></li>
    <li><a href="https://codalab.lisn.upsaclay.fr/competitions/21775" target="_blank">3D human contact prediction</a></li>
    <li><a href="https://codalab.lisn.upsaclay.fr/competitions/21781" target="_blank">Semantic 3D contact prediction from RGB images</a></li>
  </ul>
  
  <h3>Important dates</h3>
  <ul>
      <li>Challenge open: TBD</li>
      <li>Submission deadline: TBD</li>
      <li>Winner award: TBD</li>
  </ul>
  

<h2 id="organizers">Workshop Organizers</h2>
<center>

<table style="width:100%">
<p>

<tr>
  <td colspan="2"><center><a href="https://xiwang1212.github.io/homepage/" target="_blank"> <img alt src="data/xiwang.jpg" height="170"/> </a></center> </td>
  <td colspan="2"><center><a href="https://people.mpi-inf.mpg.de/~xxie/" target="_blank"> <img alt src="data/xianghui.png" height="170"/> </a></center> </td>
  <td colspan="2"><center><a href="https://is.mpg.de/~nathanasiou" target="_blank"> <img alt src="data/nathanasiou.jpg" height="170"/> </a></center> </td>
</tr>
<tr>
  <td colspan="2"> <center> <h3> Xi Wang </h3> </center></td>
  <td colspan="2"> <center> <h3> Xianghui Xie </h3> </center></td>
  <td colspan="2"> <center> <h3> Nikos Athanasiou </h3> </center></td>
</tr>
<tr>
  <td colspan="2"> <center> <font size= "2"> ETH Zurich & TUM</font></center> </td>
  <td colspan="2"> <center> <font size= "2"> Max Planck Institute for Informatics,<br>Saarland Informatics Campus</font></center> </td>
  <td colspan="2"> <center> <font size= "2"> Max-Planck-Institute for Intelligent Systems,<br>Tübingen</font></center> </td>
</tr>

<tr>
  <td colspan="2"><center><a href="https://dtzionas.com/" target="_blank"> <img alt src="data/dmitrios_tzionas.jpg" height="170"/> </a></center> </td>
  <td  colspan="2"><center><a href="https://virtualhumans.mpi-inf.mpg.de/people/Bhatnagar.html" target="_blank"><img alt src="data/bhatnagar.jpg" height="170"/> </a></center> </td>
  <td colspan="2"><center><a href="https://scholar.google.com/citations?user=BjEdv_AAAAAJ" target="_blank"> <img alt src="data/alexey_gavryushin.png" height="170"/> </a></center> </td>
</tr>
<tr>
  <td colspan="2"> <center> <h3> Dimitrios Tzionas </h3> </center></td>
  <td colspan="2"> <center> <h3> Bharat Lal Bhatnagar </h3> </center></td>
  <td colspan="2"> <center> <h3> Alexey Gavryushin </h3> </center></td>
</tr>
<tr>
  <td colspan="2"> <center> <font size= "2">University of Amsterdam</font></center> </td>
  <td colspan="2"> <center> <font size= "2"> Research Scientist, Meta Reality Labs </font></center> </td>
  <td colspan="2"> <center> <font size= "2"> ETH Zurich </font></center> </td>
</tr>

<tr>
  <td colspan="2"><center><a href="https://research.google/people/107250/" target="_blank"> <img alt src="data/thiemo_alldieck.png" height="170"/> </a></center> </td>
  <td colspan="2"><center><a href="https://ait.ethz.ch/people/mkocabas" target="_blank"><img alt src="data/muhammed_kocabas.jpg" height="170"/> </a></center> </td>
  <td colspan="2"><center><a href="https://insait.ai/prof-luc-van-gool/" target="_blank"> <img alt src="data/luc_van_gool.jpeg" height="170"/> </a></center> </td>
</tr>
<tr>
  <td colspan="2"> <center> <h3> Thiemo Alldieck  </h3> </center></td>
  <td colspan="2"> <center> <h3> Muhammed Kocabas </h3> </center></td>
  <td colspan="2"> <center> <h3> Luc Van Gool</h3> </center></td>
</tr>
<tr>
  <td colspan="2"> <center> <font size= "2"> Google DeepMind </font></center> </td>
  <td colspan="2"> <center> <font size= "2"> Max-Planck-Institute for Intelligent Systems,<br>Tübingen & Meshcapade</font></center> </td>
  <td colspan="2"> <center> <font size= "2"> INSAIT, Sofia University &quot;St. Kliment Ohridski&quot;,<br>Bulgaria & ETH Zurich</font></center> </td>
</tr>

<tr>
  <td colspan="3"><center><a href="https://people.inf.ethz.ch/marc.pollefeys/" target="_blank"> <img alt src="data/marc_pollefeys.jpg" height="170"/> </a></center> </td>
  <td colspan="3"><center><a href="https://virtualhumans.mpi-inf.mpg.de/people/pons-moll.html" target="_blank"><img alt src="data/GPM_new_crop.png" height="170"/> </a></center> </td>
</tr>
<tr>
  <td colspan="3"> <center> <h3> Marc Pollefeys  </h3> </center></td>
  <td colspan="3"> <center> <h3> Gerard Pons-Moll </h3> </center></td>
</tr>
<tr>
  <td colspan="3"> <center> <font size= "2"> ETH Zurich & Microsoft</font></center> </td>
  <td colspan="3"> <center> <font size= "2"> University of Tübingen <br>Max Planck Institute for Informatics,<br>Saarland Informatics Campus</font></center> </td>
</tr>


<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>

</p>
</table>
</center>

<h2 id="organizers">Challenge Organizers</h2>
<center>

<table style="width:100%">
<p>

<tr>
  <td colspan="2"><center><a href="https://people.mpi-inf.mpg.de/~xxie/" target="_blank"> <img alt src="data/xianghui.png" height="170"/> </a></center> </td>
  <td colspan="2"><center><a href="https://sha2nkt.github.io/" target="_blank"> <img alt src="data/shashank.jpg" height="170"/> </a></center> </td>
  <td colspan="2"><center><a href="https://is.mpg.de/~nathanasiou" target="_blank"> <img alt src="data/nathanasiou.jpg" height="170"/> </a></center> </td>
</tr>
<tr>
  <td colspan="2"> <center> <h3> Xianghui Xie </h3> </center></td>
  <td colspan="2"> <center> <h3> Shashank Tripathi </h3> </center></td>
  <td colspan="2"> <center> <h3> Nikos Athanasiou </h3> </center></td>
</tr>
<tr>
  <td colspan="2"> <center> <font size= "2">Max Planck Institute for Informatics, Saarland Informatics Campus </font></center> </td>
  <td colspan="2"> <center> <font size= "2">Max-Planck-Institute for Intelligent Systems, Tübingen </font></center> </td>
  <td colspan="2"> <center> <font size= "2"> Max-Planck-Institute for Intelligent Systems,<br>Tübingen </font></center> </td>
</tr>



<tr>
  <td colspan="2"><center><a href="https://dtzionas.com/" target="_blank"> <img alt src="data/dmitrios_tzionas.jpg" height="170"/> </a></center> </td>
  <td colspan="2"><center><a href="https://afterjourney00.github.io/" target="_blank"><img alt src="data/chengfeng_zhao.jpg" height="170"/> </a></center> </td>
  <td colspan="2"><center><a href="https://scholar.google.com/citations?user=qbovRUIAAAAJ" target="_blank"> <img alt src="data/juze_zhang.jpg" height="170"/> </a></center> </td>
</tr>
<tr>
  <td colspan="2"> <center> <h3> Dimitrios Tzionas </h3> </center></td>
  <td colspan="2"> <center> <h3> Chengfeng Zhao </h3> </center></td>
  <td colspan="2"> <center> <h3> Juze Zhang </h3> </center></td>
</tr>
<tr>
  <td colspan="2"> <center> <font size= "2">University of Amsterdam</font></center> </td>
  <td colspan="2"> <center> <font size= "2"> ShanghaiTech University, China </font></center> </td>
  <td colspan="2"> <center> <font size= "2"> ShanghaiTech University, China  </font></center> </td>
</tr>



<tr>
  <td colspan="3"><center><a href="https://www.xu-lan.com/" target="_blank"> <img alt src="data/lan_xu.jpg" height="170"/> </a></center> </td>
  <td colspan="3"><center><a href="https://virtualhumans.mpi-inf.mpg.de/people/pons-moll.html" target="_blank"> <img alt src="data/GPM_new_crop.png" height="170"/> </a></center> </td>
</tr>
<tr>
  <td colspan="3"> <center> <h3> Lan Xu </h3> </center></td>
  <td colspan="3"> <center> <h3> Gerard Pons-Moll </h3> </center></td>
</tr>
<tr>
  <td colspan="3"> <center> <font size= "2"> ShanghaiTech University, China </font></center> </td>
  <td colspan="3"> <center> <font size= "2"> University of Tübingen <br>Max Planck Institute for Informatics,<br>Saarland Informatics Campus</font></center> </td>
</tr>


</p>
</table>
</center>

<h2 id="Committee Members">Committee Members</h2>
<p>
  Nikos Athanasiou, MPI Intelligent Systems, Germany <br>
  Dimitrije Antic, University of Amsterdam, Netherlands <br>
  George Paschalidis, University of Amsterdam, Netherlands <br>
  Sammy Christen, Disney Research, Switzerland  <br>
  Zicong Fan, ETH Zurich, Switzerland <br>
  Bharat Lal Bhatnagar, Meta, Switzerland <br>
  Muhammed Kocabas, Meshcapade, Germany <br>
  Otmar Hilliges, ETH Zurich, Switzerland <br>
  Chun-Hao Paul Huang, Adobe London, UK <br>
  Yannan He, University of Tübingen, Germany <br>
  Jiaxi Jiang, ETH Zurich, Switzerland <br>
  Chuqiao Li, University of Tübingen, Germany <br>
  Bowen Wen, NVIDIA, USA <br>
  Kaichun Mo, NVIDIA, USA <br>
  Ilya Petrov, University of Tübingen, Germany <br>
  Gerard Pons-Moll, University of Tübingen and MPI, Germany <br>
  Omid Taheri, MPI-IS, Germany <br>
  Purva Tendulkar, Columbia University, USA <br>
  Anh N. Thai, Georgia Tech, USA <br>
  Shashank Tripathi, MPI Intelligent System, Germany <br>
  Dimitrios Tzionas, University of Amsterdam, Netherlands <br>
  Julien Valentin, Microsoft, Switzerland <br>
  Luc Van Gool, ETH Zurich, Switzerland <br>
  Xi Wang, ETH Zurich, Switzerland <br>
  Xianghui Xie, MPI, Germany  <br>
  Yuxuan Xue, University of Tübingen, Germany <br>
  Xiaohan Zhang, University of Tübingen, Germany <br>
  Keyang Zhou, University of Tübingen, Germany <br>
  Alexey Gavryushin, ETH Zurich, Switzerland <br>
  Marc Pollefeys, ETH Zurich, Switzerland <br>
  Chenfeng Zhao, ShanghaiTech University <br>
  Lan Xu, ShanghaiTech University <br>
  Juze Zhang, ShanghaiTech University

</p>

<h2 id="contact">Contact Info</h2>
<p>E-mail: 
<a href="mailto:rhobinchallenge@gmail.com" target="_blank">rhobinchallenge@gmail.com</a>
</p>

<h2 id="sponsors">Sponsors</h2>
<p>
    <center><h3 style="font-size: 170%; margin-bottom: 5px;">Gold Sponsors</h3></center>
    <table style="width:100%">
        <tr>
            <td> <center> <img src="data/deepmind_new.png" height="50px"/> </center></td>
        </tr>
    </table>
</p>

<h2 id="acknowledgements">Acknowledgements</h2>
<p>Website template borrowed from: 
<a href="https://futurecv.github.io/" target="_blank">https://futurecv.github.io/</a>
(Thanks to <a href="https://www.cs.cmu.edu/~dpathak/" target="_blank">Deepak Pathak</a>)
</p>


<div style="clear: both;">&nbsp;</div>
</div><br><br>

</body>
</html>
