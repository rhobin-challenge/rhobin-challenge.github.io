<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>CVPR23 RHOBIN Workshop</title>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <link href="data/default.css" rel="stylesheet" type="text/css" />
    <meta property='og:title' content='RHOBIN Workshop: Reconstruction of Human-Object Interactions @ CVPR23'/>
    <meta property='og:url' content='https://rhobin-challenge.github.io/' />
    <meta property="og:type" content="website" />
  </head>

  <body>
    <div id="header">
      <div id="logo">
	<h1>
		<center>
	    	 <span style="font-size:50%;color:#777;font-weight:normal">The first Workshop on</span><br>
             Reconstruction of Human-Object Interactions (RHOBIN)
             
		</center>
	</h1><br>
	<h2>
		<center>
      <span style="font-size:92%;color:#777;font-weight:normal">June 19 @ 
          <a href="https://cvpr2023.thecvf.com/" target="_blank">CVPR 2023</a> 
      <span style="font-size:92%;color:#777;font-weight:bold">In Person</span>
		</center>
	</h2><br>
	</div>

  <div id="menu">
    <center>
      <ul>
        <li><a href="#speakers">Speakers</a></li>
        <li><a href="./schedule.html" accesskey="2">Schedule</a></li>
        <li><a href="./papers.html" accesskey="3">Papers</a></li>
        <li><a href="#contact">Contact</a></li>
      </ul>
    </center>
</div>


<button onclick="topFunction()" id="myBtn" title="Go to top">Top</button>

<script>
//Get the button
var mybutton = document.getElementById("myBtn");

// When the user scrolls down 20px from the top of the document, show the button
window.onscroll = function() {scrollFunction()};

function scrollFunction() {
      if (document.body.scrollTop > 20 || document.documentElement.scrollTop > 20) {
              mybutton.style.display = "block";
                } else {
                        mybutton.style.display = "none";
                          }
}

// When the user clicks on the button, scroll to the top of the document
function topFunction() {
      document.body.scrollTop = 0;
        document.documentElement.scrollTop = 0;
}
</script>


	<div id="content">
<h2>News</h2>
<p>
<ul>
<li>
    [Jan 16, 2023] Workshop website launched, with <a href="#cfp">Call-for-Papers</a> and <a href="#speakers">speakers</a> announced.
</li>
</ul>
</p>


<h2>Introduction</h2>
<p>
This half-day workshop will provide a venue to present and discuss state-of-the-art research in
the reconstruction of human-object interactions from images. The focus will be on recent
developments in human-object interaction learning and its impact on 3D scene parsing, building
human-centric robotic assistants, and the general understanding of human behaviors.
</p>
<p>
Humans are an essential component of the interaction. Hence, it is crucial to estimate the
human pose, shape, and motion as well as objects that are being interacted with accurately to
achieve a realistic interaction. 3D Human Pose and Motion estimation from images or videos
have attracted a lot of interest. However, in most cases, the task does not explicitly involve
objects and the interaction with them. Whether it is 2D detection and/or monocular 3D
reconstruction, objects and humans have been studied separately. Humans are in constant
contact with the world as they move through it and interact with it. Considering the interaction
between them can marry the best of both worlds.
</p>
<p>
In this workshop, we invite papers on topics related to human-centered interaction modeling.
This could include, but is not limited to:
</p>
<p>
    <ul>
      <li>Estimation of 3D human pose and shape from a single image or video</li>
      <li>3D human motion prediction</li>
      <li>Interactive motion sequence generation</li>
      <li>Shape reconstruction from a single image</li>
      <li>Object 6-DoF pose estimation and tracking</li>
      <li>Human-centered object semantics and functionality modeling</li>
      <li>Joint reconstruction of both bodies and objects/scenes</li>
      <li>Interaction modeling between humans and objects, e.g., contact, physics properties</li>
      <li>Detection of human-object interaction semantics</li>
      <li>New datasets or benchmarks that have 3D annotations of both humans and objects/scenes </li>
</ul>
</p>

	
<h2 id="speakers">Invited Speakers (Check out the <a href="./schedule.html">Full Schedule</a>)</h2>
<center>

<table style="width:100%">
<p>

<tr>
<td><center><a href="http://people.ciirc.cvut.cz/~sivic/" target="_blank"> <img alt src="data/josef_sivic.jpg" height="170"/> </a></center> </td>
<td><center><a href="https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html" target="_blank"><img alt src="data/siyu.jpeg" height="170"/> </a></center> </td>
<td><center><a href="https://tsattler.github.io/" target="_blank"><img alt src="data/Torsten_profile_small.jpg" height="170"/> </a></center></td>
<td><center><a href="https://www.cs.utexas.edu/users/grauman/" target="_blank"> <img alt src="data/grauman.jpg" height="170"/> </a></center> </td>
</tr>
<tr>
<td> <center> <h3> Josef Sivick</h3> </center></td>
<td> <center> <h3> Prof. Siyu Tang</h3> </center></td>
<td> <center> <h3> Torsten Sattler</h3> </center></td>
<td> <center> <h3> Kristen Grauman </h3> </center></td>
</tr>
<tr>
<td> <center> <font size= "2">Czech Institute of Informatics, Robotics and Cybernetics (CIIRC)<br>Czech Technical University (CTU)</font></center> </td>
<td> <center> <font size= "2">ETH Z端rich</font></center> </td>
<td> <center> <font size= "2">Czech Institute of Informatics, Robotics and Cybernetics (CIIRC)<br>Czech Technical University (CTU)</font></center> </td>
<td> <center> <font size= "2">University of Texas Austin<br>Facebook AI Research (FAIR)</font></center> </td>
</tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>

</p>
</table>
</center>

<!-- (Check out the <a href="./papers.html">Accepted Papers</a>) -->
<h2 id="cfp">Call for Papers </h2>
<p>
  <p>We invite submissions of a maximum of 8 pages, excluding references, using the CVPR template. Submissions should follow CVPR 2023 instructions. All papers will be subject to a double-blind review process, i.e. authors must not identify themselves on the submitted papers. The reviewing process is single-stage without rebuttals.
  
    <h3>Submission Instructions</h3>
<br>
  Submissions are anonymous and should not include any author names, affiliations, and contact information in the PDF.
  <ul>
    <li>Online Submission System:  <a href="https://cmt3.research.microsoft.com/" target="_blank">https://cmt3.research.microsoft.com/</a></li>
    <li>Submission Format: <a href="https://www.google.com/url?q=https://media.icml.cc/Conferences/CVPR2023/cvpr2023-author_kit-v1_1-1.zip&sa=D&source=docs&ust=1673735151815622&usg=AOvVaw1T9itdE1rib-vg4rowRwgr" target="_blank">official CVPR template</a>  (double column; no more than 8 pages, excluding reference).</li>
  </ul>
  If you have any questions, feel free to reach out to us.
  </p>
 
  <h3>Timeline Table (11:59 PM, Pacific Time)</h3>
  <ul>
      <li>Full-paper submission deadline: March 9, 2023 </li>
      <li>Notification to authors: March 30, 2023</li>
      <li>Notification to authors: March 30, 2023</li>
      <li>Camera-ready deadline: April 6, 2023</li>
      <li>Workshop: June 19, 2023</li>
    </ul>
</p>



<h2 id="organizers">Workshop Organizers</h2>
<center>

<table style="width:100%">
<p>

<tr>
<td><center><a href="https://xiwang1212.github.io/homepage/" target="_blank"> <img alt src="data/xiwang.jpg" height="170"/> </a></center> </td>
<td><center><a href="https://cs.stanford.edu/~kaichun/" target="_blank"><img alt src="data/kaichun.jpg" height="170"/> </a></center> </td>
<td><center><a href="https://is.mpg.de/~nathanasiou" target="_blank"> <img alt src="data/nathanasiou.jpg" height="170"/> </a></center> </td>
</tr>
<tr>
<td> <center> <h3> Xi Wang </h3> </center></td>
<td> <center> <h3> Kaichun Mo </h3> </center></td>
<td> <center> <h3> Nikos Athanasiou </h3> </center></td>
</tr>
<tr>
<td> <center> <font size= "2">ETH, Zurich</font></center> </td>
<td> <center> <font size= "2">NVIDIA Research, Seattle</font></center> </td>
<td> <center> <font size= "2"> Max-Planck-Institute for Intelligen Systems, T端bingen</font></center> </td>
</tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>

<tr>
  <td><center><a href="https://research.adobe.com/person/paulchhuang/" target="_blank"> <img alt src="data/paulhuang.jpeg" height="170"/> </a></center> </td>
  <td><center><a href="https://virtualhumans.mpi-inf.mpg.de/people/pons-moll.html" target="_blank"> <img alt src="data/GPM_new_crop.png" height="170"/> </a></center> </td>
  <td><center><a href="https://ait.ethz.ch/people/hilliges/" target="_blank"> <img alt src="data/otmarhilliges.jpg" height="170"/> </a></center> </td>
</tr>
<tr>
<td> <center> <h3>  Chun-Hao (Paul) Huang </h3> </center></td>
<td> <center> <h3> Gerard Pons-Moll</h3> </center></td>
<td> <center> <h3> Otmar Hilliges  </h3> </center></td>
</tr>
<tr>
<td> <center> <font size= "2"> Adobe, London </font></center> </td>
<td> <center> <font size= "2"> University of T端bingen <br>Max Planck Institute for Informatics, Saarland Informatics Campus</font></center> </td>
<td> <center> <font size= "2"> ETH, Zurich</font></center> </td>
</tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>

</p>
</table>
</center>

<h2 id="organizers">Challenge Organizers</h2>
<center>

<table style="width:100%">
<p>

<tr>
<td><center><a href="https://people.mpi-inf.mpg.de/~xxie/" target="_blank"> <img alt src="data/xianghui.png" height="170"/> </a></center> </td>
<td><center><a href="https://virtualhumans.mpi-inf.mpg.de/people/Bhatnagar.html" target="_blank"> <img alt src="data/bhatnagar_cropped.jpg" height="170"/> </a></center> </td>
<td><center><a href="https://virtualhumans.mpi-inf.mpg.de/people/pons-moll.html" target="_blank"> <img alt src="data/GPM_new_crop.png" height="170"/> </a></center> </td>
</tr>
<tr>
<td> <center> <h3> Xianghui Xie </h3> </center></td>
<td> <center> <h3> Bharat Lal Bhatnagar </h3> </center></td>
<td> <center> <h3> Gerard Pons-Moll </h3> </center></td>
</tr>
<tr>
<td> <center> <font size= "2">Max Planck Institute for Informatics, Saarland Informatics Campus</font></center> </td>
<td> <center> <font size= "2">Max Planck Institute for Informatics, Saarland Informatics Campus</font></center> </td>
<td> <center> <font size= "2"> University of T端bingen <br>Max Planck Institute for Informatics, Saarland Informatics Campus</font></center> </td>
</tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>
<tr><td></td></tr>


</p>
</table>
</center>

<h2 id="contact">Contact Info</h2>
<p>E-mail: 
<a href="mailto:rhobinchallenge@gmail.com" target="_blank">rhobinchallenge@gmail.com</a>
</p>

<h2 id="contact">Acknowledgements</h2>
<p>Website template borrowed from: 
<a href="https://futurecv.github.io/" target="_blank">https://futurecv.github.io/</a>
(Thanks to <a href="https://www.cs.cmu.edu/~dpathak/" target="_blank">Deepak Pathak</a>)
</p>


<div style="clear: both;">&nbsp;</div>
</div><br><br>

</body>
</html>
