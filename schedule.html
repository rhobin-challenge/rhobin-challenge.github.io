<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>3rd RHOBIN Workshop@CVPR25</title>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <link href="data/default.css" rel="stylesheet" type="text/css" />
    <link rel="icon" type="image/png" href="./data/rhobin-logo.png" />
  </head>

  <body>
    <div id="header">
      <div id="logo">
        <h1>
            <center>
                <span style="font-size:50%;color:#777;font-weight:normal">The third Workshop on</span><br>
                Reconstruction of Human-Object Interactions (RHOBIN)
            </center>
        </h1><br>
        <div class="my-image-box-header">
            <center><img src="./data/rhobin-logo-no-bg.png" alt="rhobin-logo" width="128" height="168"></center>
        </div>
        <h2>
            <center>
                <span style="font-size:92%;color:#777;font-weight:normal">June 12, 08<sup>50</sup> - 12<sup>30</sup> @
                    <a href="https://cvpr2025.thecvf.com/" target="_blank">CVPR 2025</a>, Nashville TN, USA</span><br>
                <span style="font-size:130%;color:#777;font-weight:bold">Music City Center</span>
            </center>
        </h2><br>
    </div>
  
    <div id="menu">
        <center>
            <ul>
                <li class="first"><a href="./index.html" accesskey="1">Home</a></li>
            </ul>
        </center>
    </div>

    <button onclick="topFunction()" id="myBtn" title="Go to top">Top</button>

    <script>
        var mybutton = document.getElementById("myBtn");
        window.onscroll = function() {scrollFunction()};
        function scrollFunction() {
            if (document.body.scrollTop > 20 || document.documentElement.scrollTop > 20) {
                mybutton.style.display = "block";
            } else {
                mybutton.style.display = "none";
            }
        }
        function topFunction() {
            document.body.scrollTop = 0;
            document.documentElement.scrollTop = 0;
        }
    </script>

    <div id="content">
        <div id="tz_select">
            <p><h2 style="color:rgb(188, 131, 9);">Nashville Time (CDT)</h2></p>
        </div>

        <table class="schedule" id="program">
            <tr class="row_type_tt">
                <td width="80px"><b style="color:white;">Start</b></td>
                <td width="80px"><b style="color:white;">End</b></td>
                <td><b style="color:white;">Event</b></td>
            </tr>
            
            <tr class="row_type_C">
                <td><span>12 June 2025 08:50:00 CDT</span></td>
                <td><span>12 June 2025 09:00:00 CDT</span></td>
                <td>
                    <p class="poster_title" style="margin-bottom: 5px;">Opening Remarks</p>
                    <div style="margin-bottom: 20px;">Workshop Organizers</div>
                    
                </td>
            </tr>

            <tr class="row_type_A">
                <td><span>12 June 2025 09:00:00 CDT</span></td>
                <td><span>12 June 2025 09:30:00 CDT</span></td>
                <td>
                    <table style="width:100%"><tr>
                        <td style="border:none;width:20%">
                            <a href="https://fbogo.github.io" target="_blank">
                                <img alt src="data/fbogo.jpg" height="140"/>
                            </a><b>Federica Bogo</b><br>Meta Reality Labs</td>
                        <td style="border:none;width:80%">
                            <p class="poster_title">Understanding Human Motion in the Wild</p>
                            <p class="poster_abstract">Machine perception of human motion is crucial in many areas, from Robotics to Mixed Reality. However, developing perception algorithms that work in the wild is challenging, due to the complexity of human behavior and the difficulty of acquiring 3D data at scale. In this talk, we will focus on how to tackle these challenges looking at recent advances in computer vision and machine learning. First, we will look at robust motion reconstruction algorithms that, taking just monocular RGB(-D) videos as input, can accurately capture human motion even in the presence of noise and occlusions. Second, we will discuss the importance of capturing diverse human motion data at scale, introducing the Nymeria dataset. Nymeria is currently the largest multi-modal dataset for in-the-wild human motion, providing both ground-truth 3D pose annotations and motion-language descriptions. Finally, we will show how we can leverage this data to effectively reason about human motion, connecting it to natural language. We will present EgoLM, a recent multi-modal framework that leverages Large Language Models to simultaneously track and understand human motion in egocentric scenarios.</p>
                        </td>
                    </tr></table>
                </td>
            </tr>

            <tr class="row_type_B">
                <td><span>12 June 2025 09:30:00 CDT</span></td>
                <td><span>12 June 2025 10:00:00 CDT</span></td>
                <td>
                    <table style="width:100%"><tr>
                        <td style="border:none;width:20%">
                            <a href="https://ericyi.github.io" target="_blank">
                                <img alt src="data/ericyi.jpg" height="140"/>
                            </a><b>Li Yi</b><br>Tsinghua University</td>
                        <td style="border:none;width:80%">
                            <p class="poster_title">Learning Versatile Humanoid-Scene Interaction Skills from Human Motion</p>
                            <p class="poster_abstract">Equipping humanoid robots with interactive capabilities across a wide range of scenarios is a central objective in embodied artificial intelligence. However, the process of skill acquisition in humanoid robots is challenging due to their complex dynamics, high-dimensional perception and control demands, and underactuated nature. Fortunately, the morphological similarity between humanoid robots and humans offers a unique advantage: the vast repository of human interaction motion data serves as a valuable source of prior knowledge. This talk focuses on how to efficiently utilize this data to develop diverse interaction skills in humanoid robots. I will present three ways to leverage human motion data: learning through trial and error with human motion priors, learning by tracking human interactions, and learning by interacting with digital humans. These methods highlight the transformative potential of human motion data in advancing humanoid skills.</p>
                        </td>
                    </tr></table>
                </td>
            </tr>
            
            <tr class="row_type_F">
                <td><span>12 June 2025 10:00:00 CDT</span></td>
                <td><span>12 June 2025 10:30:00 CDT</span></td>
                <td>
                    <p class="poster_title">Coffee Break & Poster Session</p>
                </td>
            </tr>
            
            <tr class="row_type_B">
                <td><span>12 June 2025 10:30:00 CDT</span></td>
                <td><span>12 June 2025 11:00:00 CDT</span></td>
                <td>
                    <table style="width:100%"><tr>
                        <td style="border:none;width:20%"></td>
                        <td style="border:none;width:80%">
                            <p class="poster_title">Presentations of the RHOBIN Challenge<br>results given by the winners</p>
                        </td>
                    </tr></table>
                </td>
            </tr>



            <tr class="row_type_A">
                <td><span>12 June 2025 11:00:00 CDT</span></td>
                <td><span>12 June 2025 11:30:00 CDT</span></td>
                <td>
                    <table style="width:100%"><tr>
                        <td style="border:none;width:20%">
                            <a href="https://www.comp.nus.edu.sg/~ayao" target="_blank">
                                <img alt src="data/ayao.jpg" height="140"/>
                            </a><b>Angela Yao</b><br>National University of Singapore</td>
                        <td style="border:none;width:80%">
                            <p class="poster_title">From Hands to Feet: Contact-Driven Modelling of Human-Object-World Interactions</p>
                            <p class="poster_abstract">Understanding how humans interact with objects and the physical world is fundamental to modeling everyday behavior in 3D.  This talk focuses on contacts, between hands and objects, and between the feet and the world, as driving cues for 3D reconstruction and generation.  First, we will look at natural language-based generation of hand-object contacts, to improve hand-object reconstruction and generation.  Language provides a natural way of specifying I will then introduce our state-of-the-art approach on world-coordinate human mesh recovery based on inferred contacts such as the feet.</p>
                        </td>
                    </tr></table>
                </td>
            </tr>

            <tr class="row_type_B">
                <td><span>12 June 2025 11:30:00 CDT</span></td>
                <td><span>12 June 2025 12:00:00 CDT</span></td>
                <td>
                    <table style="width:100%"><tr>
                        <td style="border:none;width:20%">
                            <a href="https://jiajunwu.com" target="_blank">
                                <img alt src="data/jwu.jpg" height="140"/>
                            </a><b>Jiajun Wu</b><br>Stanford University</td>
                        <td style="border:none;width:80%">
                            <p class="poster_title">Title TBA</p>
                            <p class="poster_abstract">Abstract TBA</p>
                        </td>
                    </tr></table>
                </td>
            </tr>

            <tr class="row_type_A">
                <td><span>12 June 2025 12:00:00 CDT</span></td>
                <td><span>12 June 2025 12:30:00 CDT</span></td>
                <td><p class="poster_title">Panel discussion</p></td>
            </tr>
        </table>

        <h2 id="contact">Contact Info</h2>
        <p>E-mail: <a href="mailto:rhobinchallenge@gmail.com" target="_blank">rhobinchallenge@gmail.com</a></p>

        <div style="clear: both;">&nbsp;</div>
    </div><br><br>
  </body>
</html>